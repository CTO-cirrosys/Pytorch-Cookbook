{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Operation\n",
    "- 가장 기본적인 Linear layer 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:13:20.506049Z",
     "start_time": "2018-12-01T14:13:20.026922Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLinear(nn.Module): # Pytorch 모듈 중 nn 상속받기(nn에 있는 기능 사용 가능)\n",
    "    def __init__(self, input_size, output_size): # 초기화 함수\n",
    "        super(MyLinear, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size) # nn모듈에 있는 Linear함수 사용하기\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x) # x 연산하기(단순 linear)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:13:20.541092Z",
     "start_time": "2018-12-01T14:13:20.511148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3816, -0.4053],\n",
       "        [-0.7631,  0.5218],\n",
       "        [ 0.4066, -0.0672],\n",
       "        [ 0.7212, -0.8055],\n",
       "        [-1.1215,  0.2931],\n",
       "        [ 0.8403, -0.0718],\n",
       "        [-0.2360, -0.6763],\n",
       "        [ 1.1972, -0.6266],\n",
       "        [-0.7008,  0.9629],\n",
       "        [ 0.0022,  0.1982],\n",
       "        [-1.1919,  0.7312],\n",
       "        [-0.8778,  0.7412],\n",
       "        [-0.0575,  0.0174],\n",
       "        [ 0.2853,  0.4305],\n",
       "        [-0.1576, -0.1712],\n",
       "        [ 0.4393, -0.4449],\n",
       "        [-0.5364,  1.0364],\n",
       "        [-0.0297, -0.7440],\n",
       "        [-1.2823,  0.8703],\n",
       "        [-0.5651,  0.0920]], grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 2)\n",
    "\n",
    "x = torch.randn(20, 5)\n",
    "y = linear(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:13:20.570037Z",
     "start_time": "2018-12-01T14:13:20.549576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 5]), torch.Size([2])]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters 확인하기\n",
    "[p.size() for p in linear.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "- 회귀식을 예측하는 신경망 설계\n",
    "- y = 2x -> 기울기 '2' 예측하기\n",
    "- 임위의 수 100개(x)와 오차값이 더해진 결과값(2x + error)를 이용해 회귀식 예측\n",
    "- 예상 결과 : weight(기울기)는 2에 가깝고 bias(y 절편)는 0에 가깝다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:13:21.052642Z",
     "start_time": "2018-12-01T14:13:20.576026Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x232681c9f98>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrI7UgBfl/QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23265540da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 예상되는 결과\n",
    "x = torch.randn(100,1)\n",
    "y = 2*x\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:13:21.099057Z",
     "start_time": "2018-12-01T14:13:21.058131Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.2108]], requires_grad=True), Parameter containing:\n",
       " tensor([-0.1122], requires_grad=True)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 정의\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        #torch.nn.init.zeros_(self.linear.weight) # 가중치(weight)를 0으로 초기화하기\n",
    "        #torch.nn.init.zeros_(self.linear.bias) # bias를 0으로 초기화하기\n",
    "        #self.linear.weight.data.fill_(2) # 원하는 숫자로 초기화(기울기)\n",
    "        #self.linear.bias.data.fill_(0) # 원하는 숫자로 초기화(y절편)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        \n",
    "        return y\n",
    "model = MyModel(1,1)\n",
    "list(model.parameters()) # parameter 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:13:21.133493Z",
     "start_time": "2018-12-01T14:13:21.108038Z"
    }
   },
   "outputs": [],
   "source": [
    "# 정답 함수 정의\n",
    "def answer(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:13:21.172919Z",
     "start_time": "2018-12-01T14:13:21.141477Z"
    }
   },
   "outputs": [],
   "source": [
    "# train 정의\n",
    "def train(model, x, y, optim):\n",
    "    optim.zero_grad() # module안에 있는 모든 parameters 초기화\n",
    "    \n",
    "    # feed-forward\n",
    "    y_hat = model(x)\n",
    "    \n",
    "    # 에러 계산하기\n",
    "    loss = ((y - y_hat).pow(2)).sum() / x.size(0)\n",
    "    \n",
    "    # 오차값에 대한 기울기 구하기\n",
    "    loss.backward()\n",
    "    \n",
    "    # 파라미터 갱신하기\n",
    "    optimizer.step()\n",
    "    \n",
    "    # loss값 출력\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:13:21.225320Z",
     "start_time": "2018-12-01T14:13:21.179405Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[0.4410]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.4239], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 하이퍼-파라미터 설정\n",
    "batch_size = 1\n",
    "iter_size = 10000\n",
    "epoch_size = 10000\n",
    "\n",
    "model = MyModel(1,1)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.0001, momentum = 0.1)\n",
    "\n",
    "print(model, '\\n')\n",
    "[print(p) for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:13:33.084793Z",
     "start_time": "2018-12-01T14:13:21.231809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5803.3062, grad_fn=<ThAddBackward>) tensor([0.2000]) tensor([0.2348])\n",
      "tensor(71.5551, grad_fn=<ThAddBackward>) tensor([0.2000]) tensor([0.2036])\n",
      "tensor(0.8672, grad_fn=<ThAddBackward>) tensor([0.2000]) tensor([0.2003])\n",
      "tensor(0.0103, grad_fn=<ThAddBackward>) tensor([0.2000]) tensor([0.2000])\n",
      "tensor(0.0002, grad_fn=<ThAddBackward>) tensor([0.2000]) tensor([0.2000])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_size):\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for i in range(iter_size):\n",
    "        \n",
    "        x = torch.randn(batch_size, 1)\n",
    "        y = answer(x)\n",
    "        \n",
    "        loss = train(model, x, y, optimizer)\n",
    "        \n",
    "        avg_loss += loss\n",
    "    avg_loss / iter_size\n",
    "    \n",
    "    x_valid = torch.FloatTensor([[.1]])\n",
    "    y_valid = answer(x_valid)\n",
    "    \n",
    "    model.eval()\n",
    "    y_hat = model(x_valid)\n",
    "    model.train()\n",
    "    \n",
    "    print(avg_loss, y_valid.data[0], y_hat.data[0])\n",
    "    \n",
    "    if avg_loss < .001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:13:33.102259Z",
     "start_time": "2018-12-01T14:13:33.090781Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1.9999]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([5.0800e-06], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2\n",
    "- y = 3 * X1 + X2 - 2 * X3\n",
    "- 예상되는 결과 : weight = [3, 1, -2], bias = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:13:35.779287Z",
     "start_time": "2018-12-01T14:13:33.108748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters : \n",
      "Parameter containing:\n",
      "tensor([[ 0.3284, -0.5187, -0.3125]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0518], requires_grad=True)\n",
      "\n",
      "Optimizer : \n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "\n",
      "epoch : [1/1000](1000/1000) Train Loss : 3.03 tensor(-1.) tensor([-1.0643])\n",
      "epoch : [2/1000](1000/1000) Train Loss : 0.05 tensor(-1.) tensor([-1.0095])\n",
      "epoch : [3/1000](1000/1000) Train Loss : 0.00 tensor(-1.) tensor([-1.0034])\n",
      "\n",
      "\n",
      "The Parameters of model\n",
      "Parameter containing:\n",
      "tensor([[ 2.9935,  0.9957, -1.9961]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0002], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "    \n",
    "model = model(3,1)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "def generate_data(batch_size):\n",
    "    x = torch.randn(batch_size, 3)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def answer(x):\n",
    "    \n",
    "    return 3 * x[:,0] + x[:,1] - 2 * x[:,2]\n",
    "\n",
    "def loss_f(x):\n",
    "    \n",
    "    y = answer(x)\n",
    "    y_hat = model(x)\n",
    "    \n",
    "    loss = ((y.view(batch_size,1) - y_hat).pow(2)).sum() / x.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "batch_size = 3\n",
    "epoch_n = 1000\n",
    "iter_n = 1000\n",
    "\n",
    "print('Parameters : ')\n",
    "for p in model.parameters():\n",
    "    print(p)\n",
    "\n",
    "print('')\n",
    "print('Optimizer : ')\n",
    "print(optimizer)\n",
    "print('\\n')\n",
    "\n",
    "for epoch in range(1, epoch_n+1):\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for i in range(1, iter_n+1):\n",
    "        x = torch.randn(batch_size, 3)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_f(x.data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += loss\n",
    "        \n",
    "        print('\\repoch : [{}/{}]({}/{})'.format(epoch, epoch_n, i, iter_n), end = ' ')\n",
    "        \n",
    "    avg_loss = avg_loss / iter_n\n",
    "    \n",
    "    x_valid = torch.FloatTensor([[1,2,3]])\n",
    "    y_valid = answer(x_valid)\n",
    "    \n",
    "    model.eval()\n",
    "    y_hat = model(x_valid)\n",
    "    model.train()\n",
    "    \n",
    "    print('Train Loss : %.2f' %avg_loss.item(), y_valid.data[0], y_hat.data[0])\n",
    "    \n",
    "    if avg_loss < 0.001:\n",
    "        print('\\n')\n",
    "        print('The Parameters of model')\n",
    "        for p in model.parameters():\n",
    "            print(p)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3\n",
    "- y = X1^2 + X2^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T14:13:39.428508Z",
     "start_time": "2018-12-01T14:13:35.785277Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (linear1): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (linear2): Linear(in_features=10, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.001\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "tensor(5.4976, grad_fn=<DivBackward0>)\n",
      "tensor(2.4198, grad_fn=<DivBackward0>)\n",
      "tensor(1.1075, grad_fn=<DivBackward0>)\n",
      "tensor(0.4418, grad_fn=<DivBackward0>)\n",
      "tensor(0.3677, grad_fn=<DivBackward0>)\n",
      "tensor(0.1479, grad_fn=<DivBackward0>)\n",
      "tensor(0.1480, grad_fn=<DivBackward0>)\n",
      "tensor(0.1904, grad_fn=<DivBackward0>)\n",
      "tensor(0.1407, grad_fn=<DivBackward0>)\n",
      "tensor(0.2699, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# y = X1^2 + X2^2\n",
    "def answer(x):\n",
    "    \n",
    "    y = x[:,0].pow(2) + x[:,1].pow(2)\n",
    "    \n",
    "    return y\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(input_size, 10)\n",
    "        self.linear2 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = F.relu(self.linear1(x))\n",
    "        y = F.relu(self.linear2(y))\n",
    "        \n",
    "        return y\n",
    "    \n",
    "model = Model(2,1)\n",
    "print(model, '\\n')\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "print(optimizer)\n",
    "print('')\n",
    "\n",
    "batch_size = 3\n",
    "epoch_n = 100\n",
    "iter_n = 100\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    loss_avg = 0\n",
    "    \n",
    "    for i in range(iter_n):\n",
    "        \n",
    "        x = torch.randn(batch_size, 2)\n",
    "        y_hat = model(x)\n",
    "        y = answer(x).view(y_hat.shape)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_func(y_hat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_avg += loss\n",
    "    \n",
    "    loss_avg = loss_avg / iter_n\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(loss_avg)\n",
    "    \n",
    "    if loss_avg < 0.001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 새로 배운 것\n",
    "\n",
    "## 1. 서로 다른 사이즈의 두 Tensor의 size 맞춰주기\n",
    "- tensor1.view(tensor2.shape)\n",
    "- https://stackoverflow.com/questions/53569050/pytorch-how-can-i-make-same-size-of-tensor-modelx-and-answerx/53569303?noredirect=1#comment94007240_53569303"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
