{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Pytorch 8 : Basic with Sequence\n",
    "- 앞에 진행했던 Basic을 Sequence로 입력해서 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [1., 0., 0., 0., 0.],\n",
      "         [0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1., 0.]]])\n",
      "tensor([1, 0, 2, 3, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "letters = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# hihell -> ihello\n",
    "\n",
    "x_data = [0, 1, 0, 2, 3, 3]\n",
    "y_data = [1, 0, 2, 3, 3, 4]\n",
    "\n",
    "inputs = torch.zeros(1, len(x_data), len(letters))\n",
    "for i, idx in enumerate(x_data):\n",
    "    inputs[0][i][idx] = 1\n",
    "\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "print(inputs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(5, 5, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class_n = 5\n",
    "input_size = 5\n",
    "hidden_size = 5\n",
    "batch_size = 1\n",
    "seq_n = 6\n",
    "layer_n = 1\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, class_n, input_size, hidden_size, layer_n):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.class_n = class_n\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layer_n = layer_n\n",
    "        self.seq_n = seq_n\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size = 5, hidden_size = 5,\n",
    "                          batch_first = True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.layer_n, x.size(0), self.hidden_size)\n",
    "        x = x.view(x.size(0), self.seq_n, self.input_size)\n",
    "        \n",
    "        output, _ = self.rnn(x, h0)\n",
    "        \n",
    "        return output.view(-1, class_n)               \n",
    "\n",
    "model = RNN(class_n, input_size, hidden_size, layer_n)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5731,  0.4166,  0.5236, -0.3369, -0.5740],\n",
      "        [-0.3872,  0.3059,  0.1732, -0.2040, -0.1998],\n",
      "        [-0.6863,  0.3789,  0.5690, -0.3485, -0.4284],\n",
      "        [-0.6875,  0.3078,  0.5422, -0.0775, -0.2991],\n",
      "        [-0.4715, -0.0335,  0.5258, -0.2689,  0.0209],\n",
      "        [-0.4502, -0.0124,  0.3715, -0.3620, -0.0715]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eieeee Epoch : 1/100, Loss : 1.579\n",
      "eieeee Epoch : 2/100, Loss : 1.547\n",
      "eieeee Epoch : 3/100, Loss : 1.518\n",
      "eieeee Epoch : 4/100, Loss : 1.492\n",
      "eieeee Epoch : 5/100, Loss : 1.470\n",
      "iieeee Epoch : 6/100, Loss : 1.450\n",
      "iieeee Epoch : 7/100, Loss : 1.432\n",
      "ileeee Epoch : 8/100, Loss : 1.416\n",
      "ilelee Epoch : 9/100, Loss : 1.401\n",
      "ilelee Epoch : 10/100, Loss : 1.387\n",
      "ilelee Epoch : 11/100, Loss : 1.374\n",
      "ilelee Epoch : 12/100, Loss : 1.362\n",
      "ilelll Epoch : 13/100, Loss : 1.350\n",
      "ilelll Epoch : 14/100, Loss : 1.338\n",
      "ilelll Epoch : 15/100, Loss : 1.327\n",
      "ilelll Epoch : 16/100, Loss : 1.317\n",
      "ilelll Epoch : 17/100, Loss : 1.307\n",
      "ilelll Epoch : 18/100, Loss : 1.297\n",
      "ilelll Epoch : 19/100, Loss : 1.287\n",
      "ilelll Epoch : 20/100, Loss : 1.278\n",
      "ilelll Epoch : 21/100, Loss : 1.269\n",
      "ihelll Epoch : 22/100, Loss : 1.260\n",
      "ihelll Epoch : 23/100, Loss : 1.251\n",
      "ihilll Epoch : 24/100, Loss : 1.242\n",
      "ihilll Epoch : 25/100, Loss : 1.234\n",
      "ihilll Epoch : 26/100, Loss : 1.225\n",
      "ihilll Epoch : 27/100, Loss : 1.217\n",
      "ihilll Epoch : 28/100, Loss : 1.209\n",
      "ihilll Epoch : 29/100, Loss : 1.201\n",
      "ihilll Epoch : 30/100, Loss : 1.193\n",
      "ihilll Epoch : 31/100, Loss : 1.185\n",
      "ihilll Epoch : 32/100, Loss : 1.177\n",
      "ihilll Epoch : 33/100, Loss : 1.169\n",
      "ihilll Epoch : 34/100, Loss : 1.161\n",
      "ihilll Epoch : 35/100, Loss : 1.153\n",
      "ihillo Epoch : 36/100, Loss : 1.146\n",
      "ihello Epoch : 37/100, Loss : 1.138\n",
      "ihello Epoch : 38/100, Loss : 1.131\n",
      "ihello Epoch : 39/100, Loss : 1.123\n",
      "ihello Epoch : 40/100, Loss : 1.116\n",
      "ihello Epoch : 41/100, Loss : 1.109\n",
      "ihello Epoch : 42/100, Loss : 1.101\n",
      "ihello Epoch : 43/100, Loss : 1.094\n",
      "ihello Epoch : 44/100, Loss : 1.087\n",
      "ihello Epoch : 45/100, Loss : 1.080\n",
      "ihello Epoch : 46/100, Loss : 1.073\n",
      "ihello Epoch : 47/100, Loss : 1.066\n",
      "ihello Epoch : 48/100, Loss : 1.059\n",
      "ihello Epoch : 49/100, Loss : 1.052\n",
      "ihello Epoch : 50/100, Loss : 1.046\n",
      "ihello Epoch : 51/100, Loss : 1.039\n",
      "ihello Epoch : 52/100, Loss : 1.033\n",
      "ihello Epoch : 53/100, Loss : 1.026\n",
      "ihello Epoch : 54/100, Loss : 1.020\n",
      "ihello Epoch : 55/100, Loss : 1.013\n",
      "ihello Epoch : 56/100, Loss : 1.007\n",
      "ihello Epoch : 57/100, Loss : 1.001\n",
      "ihello Epoch : 58/100, Loss : 0.995\n",
      "ihello Epoch : 59/100, Loss : 0.989\n",
      "ihello Epoch : 60/100, Loss : 0.983\n",
      "ihello Epoch : 61/100, Loss : 0.977\n",
      "ihello Epoch : 62/100, Loss : 0.972\n",
      "ihello Epoch : 63/100, Loss : 0.966\n",
      "ihello Epoch : 64/100, Loss : 0.961\n",
      "ihello Epoch : 65/100, Loss : 0.955\n",
      "ihello Epoch : 66/100, Loss : 0.950\n",
      "ihello Epoch : 67/100, Loss : 0.944\n",
      "ihello Epoch : 68/100, Loss : 0.939\n",
      "ihello Epoch : 69/100, Loss : 0.934\n",
      "ihello Epoch : 70/100, Loss : 0.929\n",
      "ihello Epoch : 71/100, Loss : 0.924\n",
      "ihello Epoch : 72/100, Loss : 0.919\n",
      "ihello Epoch : 73/100, Loss : 0.915\n",
      "ihello Epoch : 74/100, Loss : 0.910\n",
      "ihello Epoch : 75/100, Loss : 0.905\n",
      "ihello Epoch : 76/100, Loss : 0.901\n",
      "ihello Epoch : 77/100, Loss : 0.896\n",
      "ihello Epoch : 78/100, Loss : 0.892\n",
      "ihello Epoch : 79/100, Loss : 0.887\n",
      "ihello Epoch : 80/100, Loss : 0.883\n",
      "ihello Epoch : 81/100, Loss : 0.879\n",
      "ihello Epoch : 82/100, Loss : 0.875\n",
      "ihello Epoch : 83/100, Loss : 0.871\n",
      "ihello Epoch : 84/100, Loss : 0.867\n",
      "ihello Epoch : 85/100, Loss : 0.863\n",
      "ihello Epoch : 86/100, Loss : 0.859\n",
      "ihello Epoch : 87/100, Loss : 0.856\n",
      "ihello Epoch : 88/100, Loss : 0.852\n",
      "ihello Epoch : 89/100, Loss : 0.848\n",
      "ihello Epoch : 90/100, Loss : 0.845\n",
      "ihello Epoch : 91/100, Loss : 0.841\n",
      "ihello Epoch : 92/100, Loss : 0.838\n",
      "ihello Epoch : 93/100, Loss : 0.835\n",
      "ihello Epoch : 94/100, Loss : 0.831\n",
      "ihello Epoch : 95/100, Loss : 0.828\n",
      "ihello Epoch : 96/100, Loss : 0.825\n",
      "ihello Epoch : 97/100, Loss : 0.822\n",
      "ihello Epoch : 98/100, Loss : 0.819\n",
      "ihello Epoch : 99/100, Loss : 0.816\n",
      "ihello Epoch : 100/100, Loss : 0.813\n"
     ]
    }
   ],
   "source": [
    "epoch_n = 100\n",
    "\n",
    "for epoch in range(epoch_n):\n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_function(outputs, labels)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    val, idx = outputs.max(1)\n",
    "    for i in idx.data:\n",
    "        print(letters[i], end = '')\n",
    "        \n",
    "    print(' Epoch : %d/100, Loss : %1.3f' %(epoch + 1, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.7061,  1.1220, -0.4812, -0.4455, -0.4390],\n",
      "        [ 0.8600,  0.1712,  0.1434, -0.6813, -0.1535],\n",
      "        [ 0.3478, -0.4307, -0.0200, -0.1877, -0.0876],\n",
      "        [-0.4548, -0.1330,  0.7126,  0.1548,  0.2003],\n",
      "        [-0.3064, -0.4344, -0.4054,  0.8540,  0.0526]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1355,  0.5127, -0.1252,  0.0140,  0.2459],\n",
      "        [-0.0911, -0.3827,  0.2478, -0.2696,  0.1706],\n",
      "        [ 0.8697,  0.3078, -0.0048,  0.3784, -0.4636],\n",
      "        [-0.3512,  0.6472,  0.3746,  0.5062, -0.2655],\n",
      "        [-0.0491, -0.3713,  0.3258,  0.2245,  0.5007]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0060,  0.2282, -0.0727, -0.3003, -0.3133], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2728, -0.3283, -0.5207,  0.2857,  0.0372], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.3651,  0.4372, -0.2112, -0.3771, -0.4384],\n",
      "        [ 0.5541, -0.6538, -0.1807, -0.6441,  0.3463],\n",
      "        [ 0.5799, -0.4453, -0.1979, -0.0918, -0.3984],\n",
      "        [-0.8246, -0.6549,  0.3858,  0.6312,  0.1640],\n",
      "        [-0.4563, -0.2114, -0.0334,  0.5247,  0.2790]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3403,  0.7079,  0.4363, -0.1476, -0.4179],\n",
      "        [ 0.2320,  0.0728,  0.7911, -0.2040,  0.7990],\n",
      "        [ 0.2963, -0.5207, -0.2082,  0.0169, -0.1907],\n",
      "        [ 0.0032,  0.2315, -0.0497, -0.0555, -0.1450],\n",
      "        [-0.1473, -0.5750, -0.1234,  0.8123,  0.1105]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0860,  0.1304, -0.4031,  0.0056, -0.5441], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1727,  0.2225, -0.4469,  0.2588, -0.3504], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글 버젼\n",
    "- 한글로 학습 시도\n",
    "- sequence의 길이를 늘려서 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T17:56:42.037829Z",
     "start_time": "2018-12-04T17:56:41.983931Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['행복', '은', '항상', '미래', '에', '있다', ' '] 7\n",
      "tensor([[[1., 0., 0., 0., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 1., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.],\n",
      "         [0., 0., 0., 1., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 1., 0., 0.],\n",
      "         [0., 0., 0., 0., 0., 0., 1.]]])\n",
      "tensor([1, 6, 2, 6, 3, 4, 6, 5])\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "my_string = '행복은 항상 미래에 있다'\n",
    "letters = [val for val, pos in okt.pos(my_string)] + [' ']\n",
    "print(letters, len(letters))\n",
    "\n",
    "# 행복은 항상 미래에 -> 은 항상 미래에 있다\n",
    "import torch\n",
    "\n",
    "x_data = [0, 1, 6, 2, 6, 3, 4, 6] # 행복은 항상 미래에\n",
    "y_data = [1, 6, 2, 6, 3, 4, 6, 5] # 은 항상 미래에 있다\n",
    "\n",
    "inputs = torch.zeros(1, len(x_data), len(letters))\n",
    "for i, idx in enumerate(x_data):\n",
    "    inputs[0][i][idx] = 1\n",
    "\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "print(inputs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T17:57:19.564786Z",
     "start_time": "2018-12-04T17:57:19.504399Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(7, 7, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class_n = len(letters) # 7\n",
    "input_size = len(letters) # 7\n",
    "hidden_size = 7 # input_size와 같다\n",
    "batch_size = 1\n",
    "seq_n = 8\n",
    "layer_n = 1\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, class_n, layer_n):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.class_n = class_n\n",
    "        self.layer_n = layer_n\n",
    "        self.seq_n = seq_n\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size = 7, hidden_size = 7, batch_first = True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.Tensor(self.layer_n, batch_size, hidden_size)\n",
    "        x = x.view(batch_size, self.seq_n, -1)\n",
    "        \n",
    "        output, _ = self.rnn(x, h0)\n",
    "        \n",
    "        return output.view(-1, self.class_n)\n",
    "\n",
    "model = RNN(input_size, hidden_size, class_n, layer_n)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T17:57:20.367295Z",
     "start_time": "2018-12-04T17:57:19.687059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "은항상은에은은은은 Epoch : 1/100, Loss = 2.094\n",
      "은에에에에  에 Epoch : 2/100, Loss = 1.864\n",
      " 에에에미래  있다 Epoch : 3/100, Loss = 1.707\n",
      "은 항상에미래  있다 Epoch : 4/100, Loss = 1.495\n",
      "    있다에 있다 Epoch : 5/100, Loss = 1.483\n",
      "은 항상 있다에 있다 Epoch : 6/100, Loss = 1.301\n",
      "항상 항상 있다에 있다 Epoch : 7/100, Loss = 1.258\n",
      "은 항상 있다에 있다 Epoch : 8/100, Loss = 1.166\n",
      " 항상항상 미래에 있다 Epoch : 9/100, Loss = 1.131\n",
      "항상 항상 있다에 있다 Epoch : 10/100, Loss = 1.062\n",
      "항상 항상 미래에 있다 Epoch : 11/100, Loss = 0.993\n",
      "은 항상 미래에 있다 Epoch : 12/100, Loss = 0.950\n",
      "  항상 미래에 있다 Epoch : 13/100, Loss = 1.047\n",
      " 미래미래 미래에 있다 Epoch : 14/100, Loss = 1.174\n",
      "은 항상 미래에 있다 Epoch : 15/100, Loss = 0.966\n",
      "은 항상 미래에 있다 Epoch : 16/100, Loss = 0.938\n",
      "은 항상 미래에 항상 Epoch : 17/100, Loss = 0.995\n",
      "은 항상 미래에 있다 Epoch : 18/100, Loss = 0.937\n",
      "은 항상 미래에 있다 Epoch : 19/100, Loss = 0.896\n",
      "은 항상 미래에 있다 Epoch : 20/100, Loss = 0.885\n",
      "은 항상 미래에 있다 Epoch : 21/100, Loss = 0.881\n",
      "은 항상 미래에 있다 Epoch : 22/100, Loss = 0.841\n",
      "은 항상 미래에 있다 Epoch : 23/100, Loss = 0.917\n",
      "항상 항상 미래에 있다 Epoch : 24/100, Loss = 0.854\n",
      "은 항상 미래에 있다 Epoch : 25/100, Loss = 0.846\n",
      "은항상항상 미래에 있다 Epoch : 26/100, Loss = 0.814\n",
      "은 항상 미래에 있다 Epoch : 27/100, Loss = 0.777\n",
      "미래에  미래에 있다 Epoch : 28/100, Loss = 1.220\n",
      "은 항상 미래에 있다 Epoch : 29/100, Loss = 0.768\n",
      "은 항상 미래에 있다 Epoch : 30/100, Loss = 0.859\n",
      "은 항상 미래에 있다 Epoch : 31/100, Loss = 0.766\n",
      "은미래항상 미래에 있다 Epoch : 32/100, Loss = 0.783\n",
      "은 항상 미래에 있다 Epoch : 33/100, Loss = 0.870\n",
      "은 항상 미래에 있다 Epoch : 34/100, Loss = 0.863\n",
      "은 항상 미래에 있다 Epoch : 35/100, Loss = 0.758\n",
      "은 항상 미래에 있다 Epoch : 36/100, Loss = 0.755\n",
      "은 항상 미래에 있다 Epoch : 37/100, Loss = 0.849\n",
      "은 항상 미래에 있다 Epoch : 38/100, Loss = 0.808\n",
      "은 항상 미래에 있다 Epoch : 39/100, Loss = 0.765\n",
      "은 항상 미래에 있다 Epoch : 40/100, Loss = 0.738\n",
      "은 항상 미래에 있다 Epoch : 41/100, Loss = 0.823\n",
      "은 항상 미래에 있다 Epoch : 42/100, Loss = 0.735\n",
      "은 항상 미래에 있다 Epoch : 43/100, Loss = 0.723\n",
      "은 항상 미래에 있다 Epoch : 44/100, Loss = 0.750\n",
      "은 항상 미래에 있다 Epoch : 45/100, Loss = 0.800\n",
      "은 있다 미래에 있다 Epoch : 46/100, Loss = 0.789\n",
      "은 항상 미래에 있다 Epoch : 47/100, Loss = 0.718\n",
      "은 항상 미래에 있다 Epoch : 48/100, Loss = 0.712\n",
      "은 항상 미래에 있다 Epoch : 49/100, Loss = 0.719\n",
      "미래항상항상 미래에 있다 Epoch : 50/100, Loss = 0.852\n",
      "은 항상 미래에 있다 Epoch : 51/100, Loss = 0.750\n",
      "은 항상 미래에 있다 Epoch : 52/100, Loss = 0.817\n",
      "은항상항상 미래에 있다 Epoch : 53/100, Loss = 0.751\n",
      "은 항상 미래에 있다 Epoch : 54/100, Loss = 0.713\n",
      "은 항상 미래에 있다 Epoch : 55/100, Loss = 0.713\n",
      "은 항상 미래에 있다 Epoch : 56/100, Loss = 0.812\n",
      "은 항상 미래에 있다 Epoch : 57/100, Loss = 0.810\n",
      "은 항상 미래에 있다 Epoch : 58/100, Loss = 0.725\n",
      "은 항상 미래에 있다 Epoch : 59/100, Loss = 0.705\n",
      "은 항상 미래에 있다 Epoch : 60/100, Loss = 0.699\n",
      "은 항상 미래에 있다 Epoch : 61/100, Loss = 0.723\n",
      "은 항상 미래에 있다 Epoch : 62/100, Loss = 0.700\n",
      "은 항상 미래에 있다 Epoch : 63/100, Loss = 0.716\n",
      "은 항상 미래에 있다 Epoch : 64/100, Loss = 0.696\n",
      "은 항상 미래에 있다 Epoch : 65/100, Loss = 0.709\n",
      "은항상항상 미래에 있다 Epoch : 66/100, Loss = 0.801\n",
      "은 항상 미래에 있다 Epoch : 67/100, Loss = 0.702\n",
      "은 항상 미래에 있다 Epoch : 68/100, Loss = 0.697\n",
      "은 항상 미래에 있다 Epoch : 69/100, Loss = 0.697\n",
      "은 항상 미래에 있다 Epoch : 70/100, Loss = 0.697\n",
      "은항상항상 미래에 있다 Epoch : 71/100, Loss = 0.800\n",
      "은항상항상 미래에 있다 Epoch : 72/100, Loss = 0.800\n",
      "은 항상 미래에 있다 Epoch : 73/100, Loss = 0.720\n",
      "은항상항상 미래에 있다 Epoch : 74/100, Loss = 0.798\n",
      "은항상항상 미래에 있다 Epoch : 75/100, Loss = 0.797\n",
      "은 항상 미래에 있다 Epoch : 76/100, Loss = 0.689\n",
      "은 항상 미래에 있다 Epoch : 77/100, Loss = 0.688\n",
      "은 항상 미래에 있다 Epoch : 78/100, Loss = 0.688\n",
      "은 항상 미래에 있다 Epoch : 79/100, Loss = 0.711\n",
      "은항상항상 미래에 있다 Epoch : 80/100, Loss = 0.768\n",
      "은 항상 미래에 있다 Epoch : 81/100, Loss = 0.685\n",
      "미래항상항상 미래에 있다 Epoch : 82/100, Loss = 0.768\n",
      "은 항상 미래에 있다 Epoch : 83/100, Loss = 0.685\n",
      "은 항상 미래에 있다 Epoch : 84/100, Loss = 0.684\n",
      "은 항상 미래에 있다 Epoch : 85/100, Loss = 0.684\n",
      "은 항상 미래에 있다 Epoch : 86/100, Loss = 0.683\n",
      "은 항상 미래에 있다 Epoch : 87/100, Loss = 0.683\n",
      "은항상항상 미래에 있다 Epoch : 88/100, Loss = 0.792\n",
      "은 항상 미래에 있다 Epoch : 89/100, Loss = 0.681\n",
      "은 항상 미래에 있다 Epoch : 90/100, Loss = 0.681\n",
      "은 항상 미래에 있다 Epoch : 91/100, Loss = 0.681\n",
      "은 항상 미래에 있다 Epoch : 92/100, Loss = 0.680\n",
      "은 항상 미래에 있다 Epoch : 93/100, Loss = 0.680\n",
      "은 항상 미래에 있다 Epoch : 94/100, Loss = 0.682\n",
      "은 항상 미래에 있다 Epoch : 95/100, Loss = 0.680\n",
      "은항상항상 미래에 있다 Epoch : 96/100, Loss = 0.786\n",
      "은항상항상 미래에 있다 Epoch : 97/100, Loss = 0.761\n",
      "은항상항상 미래에 있다 Epoch : 98/100, Loss = 0.761\n",
      "은 항상 미래에 있다 Epoch : 99/100, Loss = 0.678\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.1)\n",
    "\n",
    "epoch_n = 100\n",
    "loss_list = []\n",
    "\n",
    "for epoch in range(1, epoch_n):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    output = model(inputs)\n",
    "    \n",
    "    loss = loss_function(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    val, idx = output.max(1)\n",
    "    for i in idx.data:\n",
    "        print(letters[i], end = '')\n",
    "    print(' Epoch : %d/100, Loss = %1.3f' %(epoch, loss))\n",
    "    loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T17:57:22.290838Z",
     "start_time": "2018-12-04T17:57:22.140600Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1375469a4e0>]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1374f410588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(loss_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
