{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d89010e",
   "metadata": {},
   "source": [
    "# VGG 모델 만들기\n",
    "- 모델의 구조를 보면 MaxPool를 기준으로 block단위로 나눠져있음\n",
    "- block형태를 활용해서 모델 설계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fb8785",
   "metadata": {},
   "source": [
    "![VGG](vgg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27f4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6050d91b",
   "metadata": {},
   "source": [
    "# 단순한 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28789978",
   "metadata": {},
   "source": [
    "### 1 block 메소드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c81c165",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_block(\n",
    "    in_channel=3,\n",
    "    out_channel=64,\n",
    "    num_cnn=3\n",
    "):\n",
    "    layers = []\n",
    "    for n in range(num_cnn):\n",
    "        layers.append(nn.Conv2d(in_channel, out_channel, 3, padding=1))\n",
    "        in_channel = out_channel\n",
    "    \n",
    "    layers.append(nn.MaxPool2d(2))\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1a3bd8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = build_feature_block()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecd95983",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 16, 16])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.rand(1,3,32,32)\n",
    "model = nn.Sequential(*features)\n",
    "output = model(img)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f2ba6d",
   "metadata": {},
   "source": [
    "### VGG-11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20f4618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg11(\n",
    "    in_channel=3,\n",
    "    num_cnn_list=[1,1,2,2,2],\n",
    "    channel_list=[64,128,256,512,512],\n",
    "    num_classes=10\n",
    "):\n",
    "    features = []\n",
    "    for num_cnn, channel in zip(num_cnn_list, channel_list):\n",
    "        \n",
    "        features += build_feature_block(\n",
    "            in_channel=in_channel,\n",
    "            out_channel=channel,\n",
    "            num_cnn=num_cnn)\n",
    "        \n",
    "        in_channel = channel\n",
    "    \n",
    "    flatten = [nn.Flatten()]\n",
    "    \n",
    "    classifier = []\n",
    "    classifier += [nn.Linear(512*7*7, 4096)]\n",
    "    classifier += [nn.Linear(4096, 4096)]\n",
    "    classifier += [nn.Linear(4096, 1000)]\n",
    "    \n",
    "    layers = features + flatten + classifier\n",
    "    \n",
    "    model = nn.Sequential(*layers)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bc50c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (4): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (7): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (13): Flatten(start_dim=1, end_dim=-1)\n",
       "  (14): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (15): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (16): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = vgg11()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef49c55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "         MaxPool2d-2         [-1, 64, 112, 112]               0\n",
      "            Conv2d-3        [-1, 128, 112, 112]          73,856\n",
      "         MaxPool2d-4          [-1, 128, 56, 56]               0\n",
      "            Conv2d-5          [-1, 256, 56, 56]         295,168\n",
      "            Conv2d-6          [-1, 256, 56, 56]         590,080\n",
      "         MaxPool2d-7          [-1, 256, 28, 28]               0\n",
      "            Conv2d-8          [-1, 512, 28, 28]       1,180,160\n",
      "            Conv2d-9          [-1, 512, 28, 28]       2,359,808\n",
      "        MaxPool2d-10          [-1, 512, 14, 14]               0\n",
      "           Conv2d-11          [-1, 512, 14, 14]       2,359,808\n",
      "           Conv2d-12          [-1, 512, 14, 14]       2,359,808\n",
      "        MaxPool2d-13            [-1, 512, 7, 7]               0\n",
      "          Flatten-14                [-1, 25088]               0\n",
      "           Linear-15                 [-1, 4096]     102,764,544\n",
      "           Linear-16                 [-1, 4096]      16,781,312\n",
      "           Linear-17                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 132,863,336\n",
      "Trainable params: 132,863,336\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 68.59\n",
      "Params size (MB): 506.83\n",
      "Estimated Total Size (MB): 576.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.cuda(), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2361ee6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.rand(1,3,224,224)\n",
    "output = model(img.cuda())\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97dcad17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_block(\n",
    "    in_channel=3,\n",
    "    out_channel=64,\n",
    "    num_cnn=3\n",
    "):\n",
    "    layers = []\n",
    "    for n in range(num_cnn):\n",
    "        layers.append(nn.Conv2d(in_channel, out_channel, 3, padding=1))\n",
    "        in_channel = out_channel\n",
    "    \n",
    "    layers.append(nn.MaxPool2d(2))\n",
    "    \n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce44b5b",
   "metadata": {},
   "source": [
    "### VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4aef791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cnn_list = [2,2,3,3,3]\n",
    "num_channel_list = [64, 128, 256, 512, 512]\n",
    "\n",
    "def vgg16(\n",
    "    in_channel=3,\n",
    "    num_cnn_list=[2,2,3,3,3],\n",
    "    num_channel_list=[64,128,256,512,512],\n",
    "    classes=10\n",
    "):\n",
    "    features = []\n",
    "    for num_cnn, channel in zip(num_cnn_list, num_channel_list):\n",
    "        \n",
    "        features += build_feature_block(\n",
    "            in_channel=in_channel,\n",
    "            out_channel=channel,\n",
    "            num_cnn=num_cnn\n",
    "        )\n",
    "        \n",
    "        in_channel = channel\n",
    "        \n",
    "    flatten = [nn.Flatten()]\n",
    "    \n",
    "    classifier = []\n",
    "    classifier += [nn.Linear(512*7*7, 4096)]\n",
    "    classifier += [nn.Linear(4096, 4096)]\n",
    "    classifier += [nn.Linear(4096, classes)]\n",
    "    \n",
    "    layers = features + flatten + classifier\n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e7a04ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (18): Flatten(start_dim=1, end_dim=-1)\n",
       "  (19): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "  (20): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "  (21): Linear(in_features=4096, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_16 = vgg16(classes=1000)\n",
    "vgg_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "612ff456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "            Conv2d-2         [-1, 64, 224, 224]          36,928\n",
      "         MaxPool2d-3         [-1, 64, 112, 112]               0\n",
      "            Conv2d-4        [-1, 128, 112, 112]          73,856\n",
      "            Conv2d-5        [-1, 128, 112, 112]         147,584\n",
      "         MaxPool2d-6          [-1, 128, 56, 56]               0\n",
      "            Conv2d-7          [-1, 256, 56, 56]         295,168\n",
      "            Conv2d-8          [-1, 256, 56, 56]         590,080\n",
      "            Conv2d-9          [-1, 256, 56, 56]         590,080\n",
      "        MaxPool2d-10          [-1, 256, 28, 28]               0\n",
      "           Conv2d-11          [-1, 512, 28, 28]       1,180,160\n",
      "           Conv2d-12          [-1, 512, 28, 28]       2,359,808\n",
      "           Conv2d-13          [-1, 512, 28, 28]       2,359,808\n",
      "        MaxPool2d-14          [-1, 512, 14, 14]               0\n",
      "           Conv2d-15          [-1, 512, 14, 14]       2,359,808\n",
      "           Conv2d-16          [-1, 512, 14, 14]       2,359,808\n",
      "           Conv2d-17          [-1, 512, 14, 14]       2,359,808\n",
      "        MaxPool2d-18            [-1, 512, 7, 7]               0\n",
      "          Flatten-19                [-1, 25088]               0\n",
      "           Linear-20                 [-1, 4096]     102,764,544\n",
      "           Linear-21                 [-1, 4096]      16,781,312\n",
      "           Linear-22                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 115.30\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 643.66\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(vgg_16.cuda(), (3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9170585",
   "metadata": {},
   "source": [
    "# 파이써닉 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dde0a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eebffb3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16 = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M']\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, num_classes, batch_norm=False):\n",
    "        super(VGG, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        self.features = self.create_conv_layers(VGG16)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512*7*7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.classifier(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def create_conv_layers(self, config):\n",
    "        layers = []\n",
    "        in_channels = self.in_channels\n",
    "        batch_norm = self.batch_norm\n",
    "        \n",
    "        for out_channels in config:\n",
    "            # convolution\n",
    "            if type(out_channels) == int:\n",
    "                conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "                \n",
    "                if batch_norm:\n",
    "                    layers += [conv2d, nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True)]\n",
    "                else:\n",
    "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "                \n",
    "                in_channels = out_channels\n",
    "                \n",
    "            # maxpooling\n",
    "            else:\n",
    "                layers += [nn.MaxPool2d(2)]\n",
    "    \n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34bd1e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg16 = VGG(in_channels=3, num_classes=1000)\n",
    "vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c067a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 244, 244]           1,792\n",
      "              ReLU-2         [-1, 64, 244, 244]               0\n",
      "            Conv2d-3         [-1, 64, 244, 244]          36,928\n",
      "              ReLU-4         [-1, 64, 244, 244]               0\n",
      "         MaxPool2d-5         [-1, 64, 122, 122]               0\n",
      "            Conv2d-6        [-1, 128, 122, 122]          73,856\n",
      "              ReLU-7        [-1, 128, 122, 122]               0\n",
      "            Conv2d-8        [-1, 128, 122, 122]         147,584\n",
      "              ReLU-9        [-1, 128, 122, 122]               0\n",
      "        MaxPool2d-10          [-1, 128, 61, 61]               0\n",
      "           Conv2d-11          [-1, 256, 61, 61]         295,168\n",
      "             ReLU-12          [-1, 256, 61, 61]               0\n",
      "           Conv2d-13          [-1, 256, 61, 61]         590,080\n",
      "             ReLU-14          [-1, 256, 61, 61]               0\n",
      "           Conv2d-15          [-1, 256, 61, 61]         590,080\n",
      "             ReLU-16          [-1, 256, 61, 61]               0\n",
      "        MaxPool2d-17          [-1, 256, 30, 30]               0\n",
      "           Conv2d-18          [-1, 512, 30, 30]       1,180,160\n",
      "             ReLU-19          [-1, 512, 30, 30]               0\n",
      "           Conv2d-20          [-1, 512, 30, 30]       2,359,808\n",
      "             ReLU-21          [-1, 512, 30, 30]               0\n",
      "           Conv2d-22          [-1, 512, 30, 30]       2,359,808\n",
      "             ReLU-23          [-1, 512, 30, 30]               0\n",
      "        MaxPool2d-24          [-1, 512, 15, 15]               0\n",
      "           Conv2d-25          [-1, 512, 15, 15]       2,359,808\n",
      "             ReLU-26          [-1, 512, 15, 15]               0\n",
      "           Conv2d-27          [-1, 512, 15, 15]       2,359,808\n",
      "             ReLU-28          [-1, 512, 15, 15]               0\n",
      "           Conv2d-29          [-1, 512, 15, 15]       2,359,808\n",
      "             ReLU-30          [-1, 512, 15, 15]               0\n",
      "        MaxPool2d-31            [-1, 512, 7, 7]               0\n",
      "           Linear-32                 [-1, 4096]     102,764,544\n",
      "             ReLU-33                 [-1, 4096]               0\n",
      "           Linear-34                 [-1, 4096]      16,781,312\n",
      "             ReLU-35                 [-1, 4096]               0\n",
      "           Linear-36                 [-1, 1000]       4,097,000\n",
      "================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.68\n",
      "Forward/backward pass size (MB): 258.26\n",
      "Params size (MB): 527.79\n",
      "Estimated Total Size (MB): 786.73\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(vgg16.cuda(), (3, 244, 244))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "72b0d40c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = torch.rand(1,3,224,224)\n",
    "output = vgg16(img.cuda())\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c302cf9",
   "metadata": {},
   "source": [
    "# 정리\n",
    "- 함수를 이용한 방법과 Class를 이용한 방법\n",
    "- 파이써닉하게 설계하기 위해서는 Class 방법을 선호\n",
    "- Adaptive pooling layer를 추가해서 모든 input size에 대해 사용 가능함\n",
    "- python파일로 작성 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "432c97c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10, MNIST\n",
    "\n",
    "from model import VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86a77b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d783465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Dataset....\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import vgg16\n",
    "model = vgg16(pretrained=True)\n",
    "model.classifier[6] = nn.Linear(4096, 10)\n",
    "\n",
    "datasets = {\n",
    "    'mnist': r'C:\\Users\\gjust\\Documents\\Github\\data',\n",
    "    'cifar10': r'C:\\Users\\gjust\\Documents\\Github\\data',\n",
    "    'cifar100': r'C:\\Users\\gjust\\Documents\\Github\\data'\n",
    "}\n",
    "data_path = datasets['cifar10']\n",
    "\n",
    "# Dataset\n",
    "print('Preparing Dataset....')\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "train_set = CIFAR10(root=data_path, transform=transform, train=True, download=True)\n",
    "test_set = CIFAR10(root=data_path, transform=transform, train=False, download=True)  \n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0f40f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# optmizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "680c1a66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 3, 8, 9, 4, 0, 3, 0, 7, 4, 9, 0, 9, 5, 5, 6], device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19f638e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 8, 5, 4, 4, 8, 8, 7, 4, 3, 9, 4, 9, 9, 1, 2], device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e583e8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(outputs.argmax(axis=1) == labels).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c8c9de8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter[1/3125] --- Loss: 0.1648 --- Accuracy: 0.12\n",
      "Iter[41/3125] --- Loss: 1.755e+05 --- Accuracy: 0.12\n",
      "Iter[81/3125] --- Loss: 1.078e+05 --- Accuracy: 0.11\n",
      "Iter[121/3125] --- Loss: 7.817e+04 --- Accuracy: 0.11\n",
      "Iter[161/3125] --- Loss: 5.879e+04 --- Accuracy: 0.1\n",
      "Iter[201/3125] --- Loss: 4.709e+04 --- Accuracy: 0.11\n",
      "Iter[241/3125] --- Loss: 3.928e+04 --- Accuracy: 0.11\n",
      "Iter[281/3125] --- Loss: 3.368e+04 --- Accuracy: 0.11\n",
      "Iter[321/3125] --- Loss: 2.949e+04 --- Accuracy: 0.11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16136/3612274284.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Github\\Pytorch-Cookbook\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Github\\Pytorch-Cookbook\\pytorch\\lib\\site-packages\\torchvision\\models\\vgg.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Github\\Pytorch-Cookbook\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Github\\Pytorch-Cookbook\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    140\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Github\\Pytorch-Cookbook\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Github\\Pytorch-Cookbook\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 443\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    445\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\Github\\Pytorch-Cookbook\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    437\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m--> 439\u001b[1;33m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    440\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "model.cuda()\n",
    "iter_loss = []\n",
    "corrects = 0\n",
    "data_size = 0\n",
    "\n",
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    images, labels = images.cuda(), labels.cuda()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(images)\n",
    "    loss = loss_func(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    data_size += images.shape[0]\n",
    "    iter_loss.append(loss.item())\n",
    "\n",
    "    corrects += sum(outputs.argmax(axis=1) == labels).item()\n",
    "\n",
    "    if i % 40 == 0:\n",
    "        print(f'Iter[{i+1}/{len(train_loader)}] --- Loss: {sum(iter_loss)/data_size:0.4} --- Accuracy: {corrects/data_size:0.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b760a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader, optimizer, loss_func, device):\n",
    "    model.train()\n",
    "    iter_loss = []\n",
    "    corrects = 0\n",
    "    data_size = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images, labels = images.cuda(), labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        data_size += images.shape[0]\n",
    "        iter_loss.append(loss.item())\n",
    "        print(data_size)\n",
    "        corrects += sum(outputs.argmax(axis=1) == labels).item()\n",
    "\n",
    "        if (i+1) % 40 == 0:\n",
    "            print(f'Iter[{i+1}/{len(dataloader)}] --- Loss: {sum(iter_loss)/data_size:0.4} --- Accuracy: {corrects/data_size:0.2}')\n",
    "    return sum(iter_loss)/data_size, corrects/data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3584c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader, loss_func, device):\n",
    "    model.eval()\n",
    "    iter_loss = []\n",
    "    corrects = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        data_size = 0\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images, labels = images.cuda(), labels.cuda()\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = loss_func(outputs, labels)\n",
    "            \n",
    "            data_size += images.shape[0]\n",
    "            iter_loss.append(loss.item())\n",
    "\n",
    "            corrects += sum(outputs.argmax(axis=1) == labels).item()\n",
    "    \n",
    "    print(f'Iter[{i+1}/{len(dataloader)}] --- Loss: {sum(iter_loss)/data_size:0.4} --- Accuracy: {corrects/data_size:0.2}')\n",
    "    return sum(iter_loss)/data_size, corrects/data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e605b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Dataset....\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Preparing Model....\n"
     ]
    }
   ],
   "source": [
    "datasets = {\n",
    "    'mnist': r'C:\\Users\\gjust\\Documents\\Github\\data',\n",
    "    'cifar10': r'C:\\Users\\gjust\\Documents\\Github\\data',\n",
    "    'cifar100': r'C:\\Users\\gjust\\Documents\\Github\\data'\n",
    "}\n",
    "data_path = datasets['cifar10']\n",
    "\n",
    "# Dataset\n",
    "print('Preparing Dataset....')\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "train_set = CIFAR10(root=data_path, transform=transform, train=True, download=True)\n",
    "test_set = CIFAR10(root=data_path, transform=transform, train=False, download=True)  \n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# GPU\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model\n",
    "print('Preparing Model....')\n",
    "model = VGG('vgg16', 3, 10, False)\n",
    "model.to(device)\n",
    "\n",
    "# optmizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5822c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training....\n",
      "16\n",
      "32\n",
      "48\n",
      "64\n",
      "80\n",
      "96\n",
      "112\n",
      "128\n",
      "144\n",
      "160\n",
      "176\n",
      "192\n",
      "208\n",
      "224\n",
      "240\n",
      "256\n",
      "272\n",
      "288\n",
      "304\n",
      "320\n",
      "336\n",
      "352\n",
      "368\n",
      "384\n",
      "400\n",
      "416\n",
      "432\n",
      "448\n",
      "464\n",
      "480\n",
      "496\n",
      "512\n",
      "528\n",
      "544\n",
      "560\n",
      "576\n",
      "592\n",
      "608\n",
      "624\n",
      "640\n",
      "Iter[40/3125] --- Loss: 6.322e+07 --- Accuracy: 0.1\n",
      "656\n",
      "672\n",
      "688\n",
      "704\n",
      "720\n",
      "736\n",
      "752\n",
      "768\n",
      "784\n",
      "800\n",
      "816\n",
      "832\n",
      "848\n",
      "864\n",
      "880\n",
      "896\n",
      "912\n",
      "928\n",
      "944\n",
      "960\n",
      "976\n",
      "992\n",
      "1008\n",
      "1024\n",
      "1040\n",
      "1056\n",
      "1072\n",
      "1088\n",
      "1104\n",
      "1120\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24896/1303117527.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Training....'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Teseting....'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24896/1534143342.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, loss_func, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mdata_size\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0miter_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mcorrects\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train\n",
    "\n",
    "train_epoch_list, test_epoch_list = [], []\n",
    "train_acc_list, test_acc_list = [], []\n",
    "for e in range(20):\n",
    "    print('Training....')\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, loss_func, device)\n",
    "    print('Teseting....')\n",
    "    test_loss, test_acc = test(model, test_loader, loss_func, device)\n",
    "\n",
    "    train_epoch_list.append(train_loss)\n",
    "    test_epoch_list.append(test_loss)\n",
    "    train_acc_list.append(train_acc)\n",
    "    test_acc_list.append(test_acc)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(train_epoch_list)\n",
    "plt.plot(test_epoch_list)\n",
    "plt.plot(train_acc_list)\n",
    "plt.plot(test_acc_list)\n",
    "plt.legend(['Train_loss', 'Test_loss', 'Train_acc', 'Test_acc'])\n",
    "plt.savefig('result.jpg')\n",
    "# validation\n",
    "\n",
    "# save model\n",
    "\n",
    "print('successed!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8460a547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
