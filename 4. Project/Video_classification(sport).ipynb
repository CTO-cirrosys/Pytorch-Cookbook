{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data :  2789\n",
      "Labels of classes :  ['fencing', 'shooting', 'swimming', 'tennis', 'weight_lifting']\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,244)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0),\n",
    "    transforms.RandomCrop(size=(200,200))\n",
    "])\n",
    "mydata = ImageFolder(root='../../data/Sports/data', transform=transform)\n",
    "labels_ = mydata.classes\n",
    "\n",
    "print('Number of data : ', len(mydata))\n",
    "print('Labels of classes : ', labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idxs = np.arange(0, len(mydata))\n",
    "# np.random.shuffle(idxs)\n",
    "\n",
    "trainset, test_set = random_split(mydata, [2500, 289])\n",
    "\n",
    "train_set, val_set = random_split(trainset, [2200, 300])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=32)\n",
    "val_loader = DataLoader(val_set, batch_size=8)\n",
    "test_loader = DataLoader(test_set, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_ex = iter(train_loader).next()\n",
    "img_ex[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(tensor):\n",
    "    \n",
    "    print(tensor[1][5])\n",
    "    tensor = tensor[0][5].permute(1,2,0)\n",
    "    arr = np.array(tensor)\n",
    "    \n",
    "    plt.imshow(arr)\n",
    "    \n",
    "show_img(img_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet50(pretrained=True)\n",
    "feature_layers = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_class):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.features = feature_layers\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "        self.fc2 = nn.Linear(512, num_class)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flat(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = MyModel(num_class=6)\n",
    "\n",
    "for p in model.features.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.vgg16(pretrained=True)\n",
    "model.classifier[6] = nn.Linear(in_features=4096, out_features=len(labels_), bias=True)\n",
    "\n",
    "for p in model.features.parameters():\n",
    "    p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    output = model(img_ex[0])\n",
    "    print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=2048, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3 = models.resnet152(pretrained=True)\n",
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list is not a Module subclass",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f15796c9f7de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m classifier = nn.Sequential([\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2048\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m ])\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_item_by_idx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36madd_module\u001b[1;34m(self, name, module)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \"\"\"\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 344\u001b[1;33m             raise TypeError(\"{} is not a Module subclass\".format(\n\u001b[0m\u001b[0;32m    345\u001b[0m                 torch.typename(module)))\n\u001b[0;32m    346\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_six\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list is not a Module subclass"
     ]
    }
   ],
   "source": [
    "classifier = nn.Sequential([\n",
    "    nn.Linear(2048, 1024, bias=True),\n",
    "    nn.Linear(1024, 512, bias=True),\n",
    "    nn.Linear(512, 5, bias=True)\n",
    "])\n",
    "\n",
    "model3.fc = classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.modules.linear.Linear"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model3.fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1, verbose=True)\n",
    "\n",
    "print(optimizer)\n",
    "print(scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "EPOCH = 60\n",
    "train_loss_list, val_loss_list = [], []\n",
    "\n",
    "for e in range(EPOCH):\n",
    "    start_time = time.time()\n",
    "    train_loss = 0\n",
    "    train_correct = 0\n",
    "    \n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    \n",
    "    for i, data in enumerate(train_loader):\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(images)\n",
    "        \n",
    "        loss = loss_func(output, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.cpu().item()\n",
    "        \n",
    "        pred = output.argmax(1)\n",
    "        train_correct += (pred == labels).cpu().sum().item()\n",
    "        \n",
    "    train_acc = train_correct / len(trainset)\n",
    "    train_loss = train_loss / len(trainset)\n",
    "    train_loss_list.append(train_loss)\n",
    "    \n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            output = model(images)\n",
    "            \n",
    "            loss = loss_func(output, labels)\n",
    "            \n",
    "            val_loss += loss.cpu().item()\n",
    "            \n",
    "            val_pred = output.argmax(1)\n",
    "            val_correct += (val_pred == labels).cpu().sum().item()\n",
    "            \n",
    "        val_acc = val_correct / len(val_set)\n",
    "        val_loss = val_loss / len(val_set)\n",
    "        val_loss_list.append(val_loss)\n",
    "        \n",
    "    scheduler.step()\n",
    "    end_time = time.time()\n",
    "    info = '[EPOCH {}/{}] : train-loss = {:0.4f} | train-acc = {:0.4f} | val-loss = {:0.4f} | val-acc = {:0.4f} | time = {:0.2f}'\n",
    "    print(info.format(e+1, EPOCH, train_loss, train_acc, val_loss, val_acc, end_time-start_time))\n",
    "    \n",
    "    torch.save(model.state_dict(), './vgg16_class6.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_set, batch_size=4)\n",
    "\n",
    "model.cpu()\n",
    "model.eval()\n",
    "\n",
    "test_correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "\n",
    "        prediction = model(images)\n",
    "        \n",
    "        test_correct += (prediction.argmax(1) == labels).sum().item()\n",
    "        \n",
    "    \n",
    "    test_acc = test_correct / len(test_set)\n",
    "    print('Accuracy of Test Data : %0.2f%%' %(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_loss_list)\n",
    "plt.plot(val_loss_list)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(['Train-loss', 'Val-loss'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.resnet50(pretrained=True)\n",
    "feature_layers = nn.Sequential(*list(resnet.children())[:-1])\n",
    "\n",
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_class):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.features = feature_layers\n",
    "        \n",
    "        self.flat = nn.Flatten()\n",
    "        \n",
    "        self.fc1 = nn.Linear(2048, 512)\n",
    "        self.fc2 = nn.Linear(512, num_class)\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.flat(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "model = MyModel(num_class=6)\n",
    "model.load_state_dict(torch.load('./resnet50_normalized_1.pt'))\n",
    "\n",
    "for p in model.features.parameters():\n",
    "    p.requires_grad = False\n",
    "    \n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_trained = models.vgg16(pretrained=True)\n",
    "model_trained.classifier[6] = nn.Linear(in_features=4096, out_features=len(labels_), bias=True)\n",
    "model_trained.load_state_dict(torch.load('./weights/vgg16.pt'))\n",
    "model_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_image(img_pil, frame=None):\n",
    "    \n",
    "    plt.imshow(img_pil)\n",
    "    \n",
    "    if type(img_pil) == torch.Tensor:\n",
    "        img_pil = img_pil.squeeze(0)\n",
    "        img_pil = img_pil.permute(1,2,0)\n",
    "        img_pil = Image.fromarray(np.array(img_pil))\n",
    "        print(img_pil.shape)\n",
    "    \n",
    "    elif type(img_pil) == np.ndarray:\n",
    "        img_pil = Image.fromarray(img_pil)  \n",
    "    \n",
    "    tran = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    transed = tran(img_pil)\n",
    "    image_tensor = transed.unsqueeze(0)\n",
    "    \n",
    "    model_trained.eval()\n",
    "    pred = model_trained(image_tensor)\n",
    "    label = pred.argmax(1)\n",
    "    cls = labels_[label]\n",
    "    \n",
    "    print(cls)\n",
    "    \n",
    "    return transed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "example_img = Image.open('../../data/Sports/test/weightlift.jpg')\n",
    "output = test_image(example_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(frame):\n",
    "    norm = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "    tensor = torch.from_numpy(frame.astype(np.float32)).permute(2,0,1)\n",
    "    \n",
    "    normalized = norm(tensor)\n",
    "    \n",
    "    result = normalized.unsqueeze(0)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "-----\n",
      "[INFO] cleaning up...\n"
     ]
    }
   ],
   "source": [
    "Q = deque(maxlen=64)\n",
    "\n",
    "vs = cv2.VideoCapture('../../data/Sports/weight.mp4')\n",
    "writer = None\n",
    "(W, H) = (None, None)\n",
    "\n",
    "while True:\n",
    "    (grabbed, frame) = vs.read()\n",
    "    \n",
    "    if not grabbed:\n",
    "        break\n",
    "        \n",
    "    if W is None or H is None:\n",
    "        (H, W) = frame.shape[:2]\n",
    "        \n",
    "    output = frame.copy()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = cv2.resize(frame, (224, 224)).astype('float32')\n",
    "    frame = normalize(frame)\n",
    "    \n",
    "    preds = model_trained.cpu()(frame)\n",
    "    print('-----')\n",
    "    preds = F.softmax(preds, dim=1)[0]\n",
    "    preds = preds.detach().numpy()\n",
    "    \n",
    "    Q.append(preds)\n",
    "    \n",
    "    results = np.array(Q).mean(axis=0)\n",
    "    i = np.argmax(results)\n",
    "    \n",
    "    label = labels_[i]\n",
    "    \n",
    "    text = \"activity: {}\".format(label)\n",
    "    cv2.putText(output, text, (35, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.25, (0, 255, 0), 5)\n",
    "    \n",
    "    if writer is None:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "        writer = cv2.VideoWriter('weight_1frame1.avi', fourcc, 30, (W, H), True)\n",
    "        \n",
    "    writer.write(output)\n",
    "            \n",
    "print(\"[INFO] cleaning up...\")\n",
    "writer.release()\n",
    "vs.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model.cpu()(frame)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F.softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
