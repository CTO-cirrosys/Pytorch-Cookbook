{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LeNet5\n",
    "[Gradient-Based Learning Applied to Document Recognition](https://d1wqtxts1xzle7.cloudfront.net/61181394/Gradient_Based_Learning_Lecun20191110-2628-k1dtd1.pdf?1573449385=&response-content-disposition=inline%3B+filename%3DGradient_Based_Learning_Lecun.pdf&Expires=1609063666&Signature=cvOPVDwciyy28Kx718WuwMBl~s-PJDH6AXdTEzkW0K1npnWoTsYkXbAWFM4aj5ppegmBEyk5kFaDgjGUQwh2ax7EYPPuYF0-KTIQiaKagbpRa46eprf2OkUXLeVUPefRuYYPUk8qs-86E-VgJ2YTqnFI5VwKE5NdOd9MDmR-IGwftxsRoLg6GbYhsMMOyRfluBC9hTu2eQC8svtqC~8HvxHn1aUQO6URXAqc-6BAAYVrfH94wRSW6DvUmZYicFMtlBzp6mojBIXUHNLzjQgvboa-PxX76-5p3BeMnXzBh38YwF5qTb-W39XICSe4fH1wNSVAhzj~LH3zSxZ0azyfCw__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA)\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 요약\n",
    "- Convoultional Layer 2개\n",
    "- Fully-connected Layer 3개\n",
    "- Average Pooling Layer 2개\n",
    "- Activation Function(활성화 함수) : Tanh\n",
    "- 출력값 Softmax 사용\n",
    "- 데이터 6만개(MNIST)\n",
    "- Cost Function(손실함수) : Cross-Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.Resize((32,32)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=[0.5], std=[0.5])])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='../../data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='../../data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_dataset, val_dataset = random_split(train_dataset, (55000, 5000))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
