{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4034f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import VOCDetection\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "try:\n",
    "    from defusedxml.ElementTree import parse as ET_parse\n",
    "except ImportError:\n",
    "    from xml.etree.ElementTree import parse as ET_parse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc79ee",
   "metadata": {},
   "source": [
    "# 기본 VOCDetection Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bab04f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset VOCDetection\n",
       "    Number of datapoints: 5717\n",
       "    Root location: ../../data\n",
       "    StandardTransform\n",
       "Transform: ToTensor()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../../data'\n",
    "dataset_default = VOCDetection(root=data_path, year='2012', image_set='train', transform=transforms.ToTensor())\n",
    "dataset_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25dd4515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[1.0000, 1.0000, 1.0000,  ..., 0.7922, 0.7961, 0.8000],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.7961, 0.7961, 0.7922],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.8078, 0.8039, 0.8039],\n",
       "           ...,\n",
       "           [0.8039, 0.7608, 0.8078,  ..., 0.5373, 0.4706, 0.4667],\n",
       "           [0.7529, 0.7569, 0.7569,  ..., 0.4353, 0.4235, 0.4784],\n",
       "           [0.6980, 0.7059, 0.7255,  ..., 0.3216, 0.2863, 0.3137]],\n",
       " \n",
       "          [[1.0000, 1.0000, 1.0000,  ..., 0.8039, 0.8078, 0.8118],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.8078, 0.8078, 0.8039],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8157, 0.8157],\n",
       "           ...,\n",
       "           [0.7490, 0.7059, 0.7529,  ..., 0.5020, 0.4392, 0.4431],\n",
       "           [0.6980, 0.7020, 0.7020,  ..., 0.4157, 0.4039, 0.4667],\n",
       "           [0.6431, 0.6510, 0.6706,  ..., 0.3020, 0.2667, 0.3020]],\n",
       " \n",
       "          [[1.0000, 1.0000, 1.0000,  ..., 0.9725, 0.9765, 0.9804],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.9765, 0.9765, 0.9725],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.9882, 0.9843, 0.9843],\n",
       "           ...,\n",
       "           [0.6980, 0.6549, 0.7020,  ..., 0.4667, 0.3961, 0.3961],\n",
       "           [0.6471, 0.6510, 0.6510,  ..., 0.3922, 0.3804, 0.4314],\n",
       "           [0.5922, 0.6000, 0.6196,  ..., 0.2863, 0.2510, 0.2745]]]]),\n",
       " {'annotation': {'folder': ['VOC2012'],\n",
       "   'filename': ['2008_000008.jpg'],\n",
       "   'source': {'database': ['The VOC2008 Database'],\n",
       "    'annotation': ['PASCAL VOC2008'],\n",
       "    'image': ['flickr']},\n",
       "   'size': {'width': ['500'], 'height': ['442'], 'depth': ['3']},\n",
       "   'segmented': ['0'],\n",
       "   'object': [{'name': ['horse'],\n",
       "     'pose': ['Left'],\n",
       "     'truncated': ['0'],\n",
       "     'occluded': ['1'],\n",
       "     'bndbox': {'xmin': ['53'],\n",
       "      'ymin': ['87'],\n",
       "      'xmax': ['471'],\n",
       "      'ymax': ['420']},\n",
       "     'difficult': ['0']},\n",
       "    {'name': ['person'],\n",
       "     'pose': ['Unspecified'],\n",
       "     'truncated': ['1'],\n",
       "     'occluded': ['0'],\n",
       "     'bndbox': {'xmin': ['158'],\n",
       "      'ymin': ['44'],\n",
       "      'xmax': ['289'],\n",
       "      'ymax': ['167']},\n",
       "     'difficult': ['0']}]}}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_loader = DataLoader(dataset_default)\n",
    "iter(dataset_loader).next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8033a873",
   "metadata": {},
   "source": [
    "# Customize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e43e8a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "            'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "            'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "class myVOCDetection(VOCDetection):\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        target = self.parse_voc_xml(ET_parse(self.annotations[index]).getroot())\n",
    "        \n",
    "        targets, labels = [], []\n",
    "        for t in target['annotation']['object']:\n",
    "            bbox = [int(t['bndbox']['xmin']), \n",
    "                    int(t['bndbox']['ymin']),\n",
    "                    int(t['bndbox']['xmax']),\n",
    "                    int(t['bndbox']['ymax'])]\n",
    "            label = classes.index(t['name'])\n",
    "            \n",
    "            targets.append(bbox)\n",
    "            labels.append(label)\n",
    "            \n",
    "        labels = torch.tensor(labels)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            targets = self.transform(np.array(targets))\n",
    "        \n",
    "        return img, targets, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9f4353",
   "metadata": {},
   "source": [
    "### Single Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fb5e982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[1.0000, 1.0000, 1.0000,  ..., 0.7922, 0.7961, 0.8000],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.7961, 0.7961, 0.7922],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.8078, 0.8039, 0.8039],\n",
       "           ...,\n",
       "           [0.8039, 0.7608, 0.8078,  ..., 0.5373, 0.4706, 0.4667],\n",
       "           [0.7529, 0.7569, 0.7569,  ..., 0.4353, 0.4235, 0.4784],\n",
       "           [0.6980, 0.7059, 0.7255,  ..., 0.3216, 0.2863, 0.3137]],\n",
       " \n",
       "          [[1.0000, 1.0000, 1.0000,  ..., 0.8039, 0.8078, 0.8118],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.8078, 0.8078, 0.8039],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8157, 0.8157],\n",
       "           ...,\n",
       "           [0.7490, 0.7059, 0.7529,  ..., 0.5020, 0.4392, 0.4431],\n",
       "           [0.6980, 0.7020, 0.7020,  ..., 0.4157, 0.4039, 0.4667],\n",
       "           [0.6431, 0.6510, 0.6706,  ..., 0.3020, 0.2667, 0.3020]],\n",
       " \n",
       "          [[1.0000, 1.0000, 1.0000,  ..., 0.9725, 0.9765, 0.9804],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.9765, 0.9765, 0.9725],\n",
       "           [1.0000, 1.0000, 1.0000,  ..., 0.9882, 0.9843, 0.9843],\n",
       "           ...,\n",
       "           [0.6980, 0.6549, 0.7020,  ..., 0.4667, 0.3961, 0.3961],\n",
       "           [0.6471, 0.6510, 0.6510,  ..., 0.3922, 0.3804, 0.4314],\n",
       "           [0.5922, 0.6000, 0.6196,  ..., 0.2863, 0.2510, 0.2745]]]]),\n",
       " tensor([[[[ 53,  87, 471, 420],\n",
       "           [158,  44, 289, 167]]]], dtype=torch.int32),\n",
       " tensor([[13, 15]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pp = pprint.PrettyPrinter(width=20, indent=4)\n",
    "data_path = '../../data'\n",
    "datasets = myVOCDetection(root=data_path, year='2012', image_set='train', transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(datasets, batch_size=1)\n",
    "\n",
    "images, bboxes, labels = iter(dataloader).next()\n",
    "images, bboxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b99cc55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data : 5717\n",
      "Batch of dataloader : 1\n",
      "Shape of image : torch.Size([3, 442, 500])\n",
      "Shape of bboxes : torch.Size([1, 2, 4])\n",
      "Shape of labels : torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print('Number of data :', len(datasets))\n",
    "print('Batch of dataloader :', len(images))\n",
    "print('Shape of image :', images[0].shape)\n",
    "print('Shape of bboxes :' , bboxes[0].shape)\n",
    "print('Shape of labels :', labels[0].shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf5b09",
   "metadata": {},
   "source": [
    "### Multi Batch\n",
    "Before collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2af838c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 442, 500] at entry 0 and [3, 327, 500] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-9f5d7f648ff7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbboxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Number of data :'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    433\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'each element in list of batch should be of equal size'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 442, 500] at entry 0 and [3, 327, 500] at entry 1"
     ]
    }
   ],
   "source": [
    "# pp = pprint.PrettyPrinter(width=20, indent=4)\n",
    "data_path = '../../data'\n",
    "datasets = myVOCDetection(root=data_path, year='2012', image_set='train', transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(datasets, batch_size=2)\n",
    "\n",
    "images, bboxes, labels = iter(dataloader).next()\n",
    "\n",
    "print('Number of data :', len(datasets))\n",
    "print('Batch of dataloader :', len(images))\n",
    "print('Shape of image :', images[0].shape)\n",
    "print('Shape of bboxes :' , bboxes[0].shape)\n",
    "print('Shape of labels :', labels[0].shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f008c0",
   "metadata": {},
   "source": [
    "### Multi Batch\n",
    "After collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e5e0e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.0000, 1.0000, 1.0000,  ..., 0.7922, 0.7961, 0.8000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.7961, 0.7961, 0.7922],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.8078, 0.8039, 0.8039],\n",
       "          ...,\n",
       "          [0.8039, 0.7608, 0.8078,  ..., 0.5373, 0.4706, 0.4667],\n",
       "          [0.7529, 0.7569, 0.7569,  ..., 0.4353, 0.4235, 0.4784],\n",
       "          [0.6980, 0.7059, 0.7255,  ..., 0.3216, 0.2863, 0.3137]],\n",
       " \n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 0.8039, 0.8078, 0.8118],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.8078, 0.8078, 0.8039],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.8196, 0.8157, 0.8157],\n",
       "          ...,\n",
       "          [0.7490, 0.7059, 0.7529,  ..., 0.5020, 0.4392, 0.4431],\n",
       "          [0.6980, 0.7020, 0.7020,  ..., 0.4157, 0.4039, 0.4667],\n",
       "          [0.6431, 0.6510, 0.6706,  ..., 0.3020, 0.2667, 0.3020]],\n",
       " \n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 0.9725, 0.9765, 0.9804],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9765, 0.9765, 0.9725],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9882, 0.9843, 0.9843],\n",
       "          ...,\n",
       "          [0.6980, 0.6549, 0.7020,  ..., 0.4667, 0.3961, 0.3961],\n",
       "          [0.6471, 0.6510, 0.6510,  ..., 0.3922, 0.3804, 0.4314],\n",
       "          [0.5922, 0.6000, 0.6196,  ..., 0.2863, 0.2510, 0.2745]]]),\n",
       " tensor([[[0.3216, 0.3412, 0.3647,  ..., 0.6275, 0.6275, 0.6196],\n",
       "          [0.3490, 0.3569, 0.3569,  ..., 0.6235, 0.6196, 0.6078],\n",
       "          [0.3647, 0.3569, 0.3412,  ..., 0.6196, 0.6078, 0.5961],\n",
       "          ...,\n",
       "          [0.1804, 0.1569, 0.1255,  ..., 0.1686, 0.1804, 0.1922],\n",
       "          [0.2078, 0.1882, 0.1961,  ..., 0.1882, 0.2000, 0.2118],\n",
       "          [0.2471, 0.2235, 0.2549,  ..., 0.2314, 0.2471, 0.2627]],\n",
       " \n",
       "         [[0.4235, 0.4431, 0.4706,  ..., 0.4118, 0.4118, 0.4039],\n",
       "          [0.4510, 0.4588, 0.4627,  ..., 0.4078, 0.4039, 0.3922],\n",
       "          [0.4667, 0.4588, 0.4471,  ..., 0.4039, 0.3922, 0.3804],\n",
       "          ...,\n",
       "          [0.1569, 0.1294, 0.0863,  ..., 0.0471, 0.0588, 0.0706],\n",
       "          [0.1843, 0.1647, 0.1686,  ..., 0.0588, 0.0706, 0.0824],\n",
       "          [0.2235, 0.2000, 0.2157,  ..., 0.1020, 0.1176, 0.1333]],\n",
       " \n",
       "         [[0.5137, 0.5333, 0.5529,  ..., 0.2000, 0.2000, 0.1922],\n",
       "          [0.5412, 0.5490, 0.5451,  ..., 0.1961, 0.1922, 0.1804],\n",
       "          [0.5569, 0.5490, 0.5294,  ..., 0.1922, 0.1804, 0.1686],\n",
       "          ...,\n",
       "          [0.1961, 0.1608, 0.1216,  ..., 0.0275, 0.0392, 0.0510],\n",
       "          [0.2314, 0.2039, 0.2000,  ..., 0.0392, 0.0510, 0.0627],\n",
       "          [0.2784, 0.2471, 0.2588,  ..., 0.0824, 0.0980, 0.1137]]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "# pp = pprint.PrettyPrinter(width=20, indent=4)\n",
    "data_path = '../../data'\n",
    "datasets = myVOCDetection(root=data_path, year='2012', image_set='train', transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(datasets, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "images, bboxes, labels = iter(dataloader).next()\n",
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d44117b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data : 5717\n",
      "Batch of dataloader : 2\n",
      "Shape of image : torch.Size([3, 442, 500])\n",
      "Shape of bboxes : torch.Size([1, 2, 4])\n",
      "Shape of labels : torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print('Number of data :', len(datasets))\n",
    "print('Batch of dataloader :', len(images))\n",
    "print('Shape of image :', images[0].shape)\n",
    "print('Shape of bboxes :' , bboxes[0].shape)\n",
    "print('Shape of labels :', labels[0].shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635559ea",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d53b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n",
    "            'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n",
    "            'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "class myVOCDetection(VOCDetection):\n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.images[index]).convert('RGB')\n",
    "        target = self.parse_voc_xml(ET_parse(self.annotations[index]).getroot())\n",
    "        \n",
    "        targets, labels = [], []\n",
    "        for t in target['annotation']['object']:\n",
    "            bbox = [int(t['bndbox']['xmin']), \n",
    "                    int(t['bndbox']['ymin']),\n",
    "                    int(t['bndbox']['xmax']),\n",
    "                    int(t['bndbox']['ymax'])]\n",
    "            label = classes.index(t['name'])\n",
    "            \n",
    "            targets.append(bbox)\n",
    "            labels.append(label)\n",
    "            \n",
    "        labels = torch.tensor(labels)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            targets = self.transform(np.array(targets))\n",
    "        \n",
    "        return img, targets, labels\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "data_path = '../../data'\n",
    "datasets = myVOCDetection(root=data_path, year='2012', image_set='train', transform=transforms.ToTensor())\n",
    "dataloader = DataLoader(datasets, batch_size=2, collate_fn=collate_fn)\n",
    "\n",
    "images, bboxes, labels = iter(dataloader).next()\n",
    "images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
