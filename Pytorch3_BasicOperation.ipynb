{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Operation\n",
    "- 가장 기본적인 Linear layer 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MyLinear(nn.Module): # Pytorch 모듈 중 nn 상속받기(nn에 있는 기능 사용 가능)\n",
    "    def __init__(self, input_size, output_size): # 초기화 함수\n",
    "        super(MyLinear, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size) # nn모듈에 있는 Linear함수 사용하기\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x) # x 연산하기(단순 linear)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2227, -0.1943],\n",
       "        [ 1.2512,  0.9747],\n",
       "        [-0.6487,  0.0689],\n",
       "        [-0.2078, -0.3891],\n",
       "        [-0.2994, -0.0528],\n",
       "        [ 0.6253,  0.6000],\n",
       "        [ 0.5177, -0.6912],\n",
       "        [ 0.9401, -0.3605],\n",
       "        [ 0.9108, -0.3526],\n",
       "        [-0.1123, -0.2743],\n",
       "        [ 0.6785,  0.6036],\n",
       "        [ 0.2392,  0.1490],\n",
       "        [ 0.8951, -0.0982],\n",
       "        [-0.6039, -0.3363],\n",
       "        [ 0.7697,  0.1928],\n",
       "        [-0.4583, -0.4937],\n",
       "        [ 0.8551,  0.2797],\n",
       "        [ 0.9998, -0.2335],\n",
       "        [ 0.5860, -0.4384],\n",
       "        [-0.7046, -0.3564]], grad_fn=<ThAddmmBackward>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = MyLinear(5, 2)\n",
    "\n",
    "x = torch.randn(20, 5)\n",
    "y = linear(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([2, 5]), torch.Size([2])]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters 확인하기\n",
    "[p.size() for p in linear.parameters()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "- 회귀식을 예측하는 신경망 설계\n",
    "- y = 2x -> 기울기 '2' 예측하기\n",
    "- 임위의 수 100개(x)와 오차값이 더해진 결과값(2x + error)를 이용해 회귀식 예측\n",
    "- 예상 결과 : weight(기울기)는 2에 가깝고 bias(y 절편)는 0에 가깝다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17914ae9a90>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17914ac9a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 예상되는 결과\n",
    "x = torch.randn(100,1)\n",
    "y = 2*x\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.9700,  0.3831, -0.6484]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1020], requires_grad=True)]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 정의\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        #torch.nn.init.zeros_(self.linear.weight) # 가중치(weight)를 0으로 초기화하기\n",
    "        #torch.nn.init.zeros_(self.linear.bias) # bias를 0으로 초기화하기\n",
    "        #self.linear.weight.data.fill_(2) # 원하는 숫자로 초기화(기울기)\n",
    "        #self.linear.bias.data.fill_(0) # 원하는 숫자로 초기화(y절편)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.linear(x)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "list(model.parameters()) # parameter 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 함수 정의\n",
    "def answer(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 정의\n",
    "def train(model, x, y, optim):\n",
    "    optim.zero_grad() # module안에 있는 모든 parameters 초기화\n",
    "    \n",
    "    # feed-forward\n",
    "    y_hat = model(x)\n",
    "    \n",
    "    # 에러 계산하기\n",
    "    loss = ((y - y_hat).pow(2)).sum() / x.size(0)\n",
    "    \n",
    "    # 오차값에 대한 기울기 구하기\n",
    "    loss.backward()\n",
    "    \n",
    "    # 파라미터 갱신하기\n",
    "    optimizer.step()\n",
    "    \n",
    "    # loss값 출력\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModel(\n",
      "  (linear): Linear(in_features=1, out_features=1, bias=True)\n",
      ") \n",
      "\n",
      "Parameter containing:\n",
      "tensor([[-0.1749]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.6907], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# 하이퍼-파라미터 설정\n",
    "batch_size = 1\n",
    "iter_size = 10000\n",
    "epoch_size = 10000\n",
    "\n",
    "model = MyModel(1,1)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.0001, momentum = 0.1)\n",
    "\n",
    "print(model, '\\n')\n",
    "[print(p) for p in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3128]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(11592.7695, grad_fn=<ThAddBackward>) tensor([0.2000]) tensor([0.2426])\n",
      "tensor(127.3160, grad_fn=<ThAddBackward>) tensor([0.2000]) tensor([0.2046])\n",
      "tensor(1.4442, grad_fn=<ThAddBackward>) tensor([0.2000]) tensor([0.2005])\n",
      "tensor(0.0181, grad_fn=<ThAddBackward>) tensor([0.2000]) tensor([0.2001])\n",
      "tensor(0.0003, grad_fn=<ThAddBackward>) tensor([0.2000]) tensor([0.2000])\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_size):\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for i in range(iter_size):\n",
    "        \n",
    "        x = torch.randn(batch_size, 1)\n",
    "        y = answer(x)\n",
    "        \n",
    "        loss = train(model, x, y, optimizer)\n",
    "        \n",
    "        avg_loss += loss\n",
    "    avg_loss / iter_size\n",
    "    \n",
    "    x_valid = torch.FloatTensor([[.1]])\n",
    "    y_valid = answer(x_valid)\n",
    "    \n",
    "    model.eval()\n",
    "    y_hat = model(x_valid)\n",
    "    model.train()\n",
    "    \n",
    "    print(avg_loss, y_valid.data[0], y_hat.data[0])\n",
    "    \n",
    "    if avg_loss < .001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1.9999]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([9.8639e-06], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2\n",
    "- y = 3 * X1 + X2 - 2 * X3\n",
    "- 예상되는 결과 : weight = [3, 1, -2], bias = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-01T08:43:45.095313Z",
     "start_time": "2018-12-01T08:43:22.923000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters : \n",
      "Parameter containing:\n",
      "tensor([[0.4708, 0.5305, 0.3887]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.4473], requires_grad=True)\n",
      "\n",
      "Optimizer : \n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n",
      "\n",
      "\n",
      "epoch : [1/1000](1000/1000) Train Loss : 10.60 tensor(-1.) tensor([2.3876])\n",
      "epoch : [2/1000](1000/1000) Train Loss : 6.85 tensor(-1.) tensor([1.7398])\n",
      "epoch : [3/1000](1000/1000) Train Loss : 4.63 tensor(-1.) tensor([1.2551])\n",
      "epoch : [4/1000](1000/1000) Train Loss : 3.03 tensor(-1.) tensor([0.8546])\n",
      "epoch : [5/1000](1000/1000) Train Loss : 2.04 tensor(-1.) tensor([0.5102])\n",
      "epoch : [6/1000](1000/1000) Train Loss : 1.40 tensor(-1.) tensor([0.2436])\n",
      "epoch : [7/1000](1000/1000) Train Loss : 0.92 tensor(-1.) tensor([0.0021])\n",
      "epoch : [8/1000](1000/1000) Train Loss : 0.62 tensor(-1.) tensor([-0.1810])\n",
      "epoch : [9/1000](1000/1000) Train Loss : 0.41 tensor(-1.) tensor([-0.3288])\n",
      "epoch : [10/1000](1000/1000) Train Loss : 0.28 tensor(-1.) tensor([-0.4467])\n",
      "epoch : [11/1000](1000/1000) Train Loss : 0.18 tensor(-1.) tensor([-0.5457])\n",
      "epoch : [12/1000](1000/1000) Train Loss : 0.12 tensor(-1.) tensor([-0.6223])\n",
      "epoch : [13/1000](1000/1000) Train Loss : 0.08 tensor(-1.) tensor([-0.7050])\n",
      "epoch : [14/1000](1000/1000) Train Loss : 0.05 tensor(-1.) tensor([-0.7555])\n",
      "epoch : [15/1000](1000/1000) Train Loss : 0.04 tensor(-1.) tensor([-0.7974])\n",
      "epoch : [16/1000](1000/1000) Train Loss : 0.02 tensor(-1.) tensor([-0.8332])\n",
      "epoch : [17/1000](1000/1000) Train Loss : 0.02 tensor(-1.) tensor([-0.8654])\n",
      "epoch : [18/1000](1000/1000) Train Loss : 0.01 tensor(-1.) tensor([-0.8868])\n",
      "epoch : [19/1000](1000/1000) Train Loss : 0.01 tensor(-1.) tensor([-0.9054])\n",
      "epoch : [20/1000](1000/1000) Train Loss : 0.01 tensor(-1.) tensor([-0.9216])\n",
      "epoch : [21/1000](1000/1000) Train Loss : 0.00 tensor(-1.) tensor([-0.9375])\n",
      "epoch : [22/1000](1000/1000) Train Loss : 0.00 tensor(-1.) tensor([-0.9483])\n",
      "epoch : [23/1000](1000/1000) Train Loss : 0.00 tensor(-1.) tensor([-0.9578])\n",
      "epoch : [24/1000](1000/1000) Train Loss : 0.00 tensor(-1.) tensor([-0.9656])\n",
      "epoch : [25/1000](1000/1000) Train Loss : 0.00 tensor(-1.) tensor([-0.9714])\n",
      "\n",
      "\n",
      "The Parameters of model\n",
      "Parameter containing:\n",
      "tensor([[ 2.9836,  0.9967, -1.9838]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0031], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class model(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(model, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        y = self.linear(x)\n",
    "        return y\n",
    "    \n",
    "model = model(3,1)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.001)\n",
    "\n",
    "def generate_data(batch_size):\n",
    "    x = torch.randn(batch_size, 3)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def answer(x):\n",
    "    \n",
    "    return 3 * x[:,0] + x[:,1] - 2 * x[:,2]\n",
    "\n",
    "def loss_f(x):\n",
    "    \n",
    "    y = answer(x)\n",
    "    y_hat = model(x)\n",
    "    \n",
    "    loss = ((y.view(batch_size,1) - y_hat).pow(2)).sum() / x.size(0)\n",
    "    \n",
    "    return loss\n",
    "\n",
    "batch_size = 3\n",
    "epoch_n = 1000\n",
    "iter_n = 1000\n",
    "\n",
    "print('Parameters : ')\n",
    "for p in model.parameters():\n",
    "    print(p)\n",
    "\n",
    "print('')\n",
    "print('Optimizer : ')\n",
    "print(optimizer)\n",
    "print('\\n')\n",
    "\n",
    "for epoch in range(1, epoch_n+1):\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for i in range(1, iter_n+1):\n",
    "        x = torch.randn(batch_size, 3)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = loss_f(x.data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss += loss\n",
    "        \n",
    "        print('\\repoch : [{}/{}]({}/{})'.format(epoch, epoch_n, i, iter_n), end = ' ')\n",
    "        \n",
    "    avg_loss = avg_loss / iter_n\n",
    "    \n",
    "    x_valid = torch.FloatTensor([[1,2,3]])\n",
    "    y_valid = answer(x_valid)\n",
    "    \n",
    "    model.eval()\n",
    "    y_hat = model(x_valid)\n",
    "    model.train()\n",
    "    \n",
    "    print('Train Loss : %.2f' %avg_loss.item(), y_valid.data[0], y_hat.data[0])\n",
    "    \n",
    "    if avg_loss < 0.001:\n",
    "        print('\\n')\n",
    "        print('The Parameters of model')\n",
    "        for p in model.parameters():\n",
    "            print(p)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
