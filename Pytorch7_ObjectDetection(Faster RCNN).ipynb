{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection with Faster R-CNN\n",
    "- Faster R-CNN을 이용해 물체인식 시도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:56:40.201476Z",
     "start_time": "2019-11-27T14:56:36.604529Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bbox 예시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:56:40.333085Z",
     "start_time": "2019-11-27T14:56:40.289109Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 800, 800])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = torch.zeros((1,3,800,800))\n",
    "image.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:56:40.450019Z",
     "start_time": "2019-11-27T14:56:40.395049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 20.,  30., 400., 500.],\n",
       "        [300., 400., 500., 600.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbox = torch.FloatTensor([[20, 30, 400, 500], [300, 400, 500, 600]])\n",
    "bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:56:40.497990Z",
     "start_time": "2019-11-27T14:56:40.484997Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 20.,  30., 400., 500.],\n",
       "       [300., 400., 500., 600.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:56:40.879770Z",
     "start_time": "2019-11-27T14:56:40.531970Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.patches.Rectangle at 0x21a0f2e6f98>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensor = image\n",
    "tensor = tensor.squeeze()\n",
    "tensor = tensor.permute(1,2,0)\n",
    "img = np.array(tensor)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(img)\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "rect1 = patches.Rectangle((20,30),380,470, facecolor='none', edgecolor='r')\n",
    "rect2 = patches.Rectangle((300,400),100,200, facecolor='none', edgecolor='r')\n",
    "\n",
    "ax.add_patch(rect1)\n",
    "ax.add_patch(rect2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backbone -> VGG16\n",
    "- inpput size == 800 \n",
    "- output size == 800//16 = 50\n",
    "- input과 output의 size를 위와 같도록 model 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:56:47.556940Z",
     "start_time": "2019-11-27T14:56:47.543945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 800, 800])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 확인을 위해 dummy image 생성\n",
    "\n",
    "dummy_img = torch.zeros((1,3,800,800)).float()\n",
    "dummy_img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:56:51.992391Z",
     "start_time": "2019-11-27T14:56:48.520390Z"
    }
   },
   "outputs": [],
   "source": [
    "# VGG16 모델 아키텍처 보기\n",
    "import torchvision\n",
    "\n",
    "model_vgg16 = torchvision.models.vgg16(pretrained = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:56:59.736948Z",
     "start_time": "2019-11-27T14:56:59.726955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
       " ReLU(inplace=True),\n",
       " MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_list = list(model_vgg16.features)\n",
    "layer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:57:03.099018Z",
     "start_time": "2019-11-27T14:57:03.091027Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 800, 800])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_img.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:57:07.657402Z",
     "start_time": "2019-11-27T14:57:07.381560Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 800, 800])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_list[0](dummy_img).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:58:24.326407Z",
     "start_time": "2019-11-27T14:58:05.652116Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "# output_size가 50이 될때까지 필요한 layer \n",
    "\n",
    "require_layers = []\n",
    "for layer in layer_list:\n",
    "    dummy_img = layer(dummy_img)\n",
    "    if dummy_img.size()[2] < 800//16:\n",
    "        break\n",
    "    require_layers.append(layer)\n",
    "    output_channel = dummy_img.size()[1]\n",
    "    \n",
    "print(output_channel)\n",
    "print(len(require_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:58:26.497152Z",
     "start_time": "2019-11-27T14:58:26.468170Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "faster_rcnn_feature_extractor = nn.Sequential(*require_layers)\n",
    "faster_rcnn_feature_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T14:58:52.502225Z",
     "start_time": "2019-11-27T14:58:32.693601Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "output_feature = faster_rcnn_feature_extractor(image)\n",
    "print(output_feature.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Anchor box(방법 확인, 실제 만드는 건 뒤에)\n",
    "- 각 비율, 크기마다 다른 anchor box 생성 -> 0.5, 1, 2 / 8, 16, 32\n",
    "- 각 box는 x1,y1,x2,y2를 가지고 있음 -> size == (9,4)\n",
    "- 1개의 feature map에 있는 1개의 픽셀에 대해 생성\n",
    "> Now every pixel in the output feature map maps to corresponding 16 * 16 pixels in the image. This is shown in the below image<br>\n",
    "-> 800x800에서 50x50으로 축소했으니 원본에서 16x16사이즈의 anchor box가 결국 feature map에서의 1개의 픽셀이 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:01:33.256966Z",
     "start_time": "2019-11-27T15:01:33.231976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio = [0.5, 1, 2]\n",
    "anchor_scales = [8,16,32]\n",
    "\n",
    "anchor_base = np.zeros((len(ratio) * len(anchor_scales), 4))\n",
    "anchor_base # feature map 1개의 픽셀에 대해 anchor box가 가질 수 있는 경우 9가지(비율3 x 크기3, x1 y1 x2 y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-26T12:46:33.541282Z",
     "start_time": "2019-11-26T12:46:33.374377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.50966799187809 181.01933598375618\n",
      "0\n",
      "181.01933598375618 362.03867196751236\n",
      "1\n",
      "362.03867196751236 724.0773439350247\n",
      "2\n",
      "128.0 128.0\n",
      "3\n",
      "256.0 256.0\n",
      "4\n",
      "512.0 512.0\n",
      "5\n",
      "181.01933598375618 90.50966799187809\n",
      "6\n",
      "362.03867196751236 181.01933598375618\n",
      "7\n",
      "724.0773439350247 362.03867196751236\n",
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ -37.254834  ,  -82.50966799,   53.254834  ,   98.50966799],\n",
       "       [ -82.50966799, -173.01933598,   98.50966799,  189.01933598],\n",
       "       [-173.01933598, -354.03867197,  189.01933598,  370.03867197],\n",
       "       [ -56.        ,  -56.        ,   72.        ,   72.        ],\n",
       "       [-120.        , -120.        ,  136.        ,  136.        ],\n",
       "       [-248.        , -248.        ,  264.        ,  264.        ],\n",
       "       [ -82.50966799,  -37.254834  ,   98.50966799,   53.254834  ],\n",
       "       [-173.01933598,  -82.50966799,  189.01933598,   98.50966799],\n",
       "       [-354.03867197, -173.01933598,  370.03867197,  189.01933598]])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# anchor base 채우기\n",
    "sub_sample = 16\n",
    "ctr_x = sub_sample / 2\n",
    "ctr_y = sub_sample / 2\n",
    "\n",
    "for i in range(len(ratio)):\n",
    "    for j in range(len(anchor_scales)):\n",
    "        h = sub_sample * anchor_scales[j] * np.sqrt(ratio[i])\n",
    "        w = sub_sample * anchor_scales[j] * np.sqrt(1/ratio[i])\n",
    "        print(h,w)\n",
    "        \n",
    "        index = i * len(anchor_scales) + j\n",
    "        print(index)\n",
    "        \n",
    "        anchor_base[index, 0] = ctr_y - h / 2\n",
    "        anchor_base[index, 1] = ctr_x - w / 2\n",
    "        anchor_base[index, 2] = ctr_y + h / 2\n",
    "        anchor_base[index, 3] = ctr_x + w / 2\n",
    "        \n",
    "anchor_base\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**위 작업은 1개의 feature map에 대하여 생성한 anchor box**\n",
    "- image 사이즈에서 벗어나거나 큰 anchor box는 negaitve values로 취급, 나중에 loss계산할 때 빼준다(-1로 바꿔줌)\n",
    "- feature map이 50x50임으로 각 픽셀에 대해 anchor box 생성하면 -> 17500(50x50x9)<br>\n",
    "(1개의 픽셀에 9가지의 anchor box 생성)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature map에 있는 모든 pixel에 대하여 anchor box 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모든 픽셀에 대한 센터값 모음 생성(ctr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:07:04.131059Z",
     "start_time": "2019-11-27T15:07:04.085086Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2500, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모든 픽셀에 대해 센터값(ctr_x, ctr_y) 생성\n",
    "\n",
    "feature_size = 800//16\n",
    "ctr_x = np.arange(16, (feature_size+1)*16, 16) # image 원본에서의 센터값x\n",
    "ctr_y = np.arange(16, (feature_size+1)*16, 16) # image 원본에서의 센터값y\n",
    "\n",
    "index = 0\n",
    "ctr = np.zeros((len(ctr_x) * len(ctr_y), 2))\n",
    "for x in range(len(ctr_x)):\n",
    "    for y in range(len(ctr_y)):\n",
    "        ctr[index, 1] = ctr_x[x] - 8\n",
    "        ctr[index, 0] = ctr_y[y] - 8\n",
    "        index += 1\n",
    "        \n",
    "ctr.shape # 1개의 image에 대한 모든 anchor box의 중심점의 모음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 센터값에 대당하는 anchor box 생성(위와 다르게 실제로 생성)\n",
    "- 1개의 feature map에 해당하는 모든 anchor box 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:15:27.595536Z",
     "start_time": "2019-11-27T15:15:27.334683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 4)\n"
     ]
    }
   ],
   "source": [
    "anchors = np.zeros((feature_size * feature_size * 9, 4)) # 각 anchor box에 대한 y1, x1, y2, x2\n",
    "sub_sample = 16\n",
    "index = 0\n",
    "for c in ctr: # y,x\n",
    "    ctr_y, ctr_x = c\n",
    "    for i in range(len(ratio)):\n",
    "        for j in range(len(anchor_scales)):\n",
    "            h = sub_sample * anchor_scales[j] * np.sqrt(ratio[i])\n",
    "            w = sub_sample * anchor_scales[j] * np.sqrt(1/ratio[i])\n",
    "            \n",
    "            anchors[index, 0] = ctr_y - h/2 # y1\n",
    "            anchors[index, 1] = ctr_x - w/2 # x1\n",
    "            anchors[index, 2] = ctr_y + h/2 # y2\n",
    "            anchors[index, 3] = ctr_x + w/2 # x2\n",
    "            index += 1\n",
    "print(anchors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모든 Anchor box에 Labels과 location 할당하기\n",
    "**두 종류의 Anchor에는 'Positive label'을 할당**\n",
    "1. 정답(Ground-Truth)과 비교했을 때 IOU(Intersection-over-Union)값이 가장 큰 Anchor/Anchors\n",
    "2. 정답(Ground-Truth)과 비교했을 때 IOU(Intersection-over-Union)값이 **0.7** 이상일 때\n",
    "> **Object가 1개에 대하여 여러 Anchor box에 Positive label을 할당하는 경우가 있다.**\n",
    "\n",
    "**아래와 같은 Anchor에는 'Negative label'을 할당**\n",
    "1. 정답(Ground-Truth)과 비교했을 때 IOU(Intersection-over-Union)값이 **0.3**보다 작을 때\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:15:28.793848Z",
     "start_time": "2019-11-27T15:15:28.784873Z"
    }
   },
   "outputs": [],
   "source": [
    "# 예시 Ground-Truth값(정답값 2개 objects)\n",
    "bbox = np.asarray([[20, 30, 400, 500], [300, 400, 500, 600]], dtype=np.float32) # y1, x1, y2, x2\n",
    "labels = np.asarray([6, 8], dtype=np.int8) # 0값은 배경을 나타냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유효한(valid) Anchor box의 index 찾기\n",
    "- Anchor box가 image보다 크거나 범위에서 벗어나면 유효하지 않음\n",
    "- 즉, Anchor box의 x1 y1 x2 y2 값들이 모두 0~800 사이에 있어야 유효한 box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:15:30.005153Z",
     "start_time": "2019-11-27T15:15:29.984162Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n",
      "[ 13.49033201  10.745166   194.50966799 101.254834  ]\n"
     ]
    }
   ],
   "source": [
    "index_inside = np.where((anchors[:,0] >= 0) &\n",
    "                        (anchors[:,1] >= 0) &\n",
    "                        (anchors[:,2] <= 800) &\n",
    "                        (anchors[:,3] <= 800))[0]\n",
    "print(index_inside.shape)\n",
    "print(anchors[index_inside[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label 행렬 만들기\n",
    "- 여기서 label은 물체가 있는지 없는지(objectness)만 판별\n",
    "- 따라서 1또는 -1로 표시(1은 물체가 있고 -1은 background)\n",
    "- 각 anchor box별로 물체 유무에 따라 1 or -1\n",
    "> **유효한 Anchor box 개수 = label 행렬 크기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:17:05.343118Z",
     "start_time": "2019-11-27T15:17:05.331126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8940,)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = np.empty_like(index_inside)\n",
    "label.fill(-1)\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 유효한 Anchor box 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:17:10.264965Z",
     "start_time": "2019-11-27T15:17:10.253973Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8940, 4)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_anchor_boxes = anchors[index_inside]\n",
    "valid_anchor_boxes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IOU 계산\n",
    "- 위에서 임의로 정의한 bbox를 정답(Ground-Truth)이라고 가정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:20:26.697227Z",
     "start_time": "2019-11-27T15:20:26.133553Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8940, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ious = np.zeros((len(valid_anchor_boxes), 2)) # 각 Anchor box와 Ground-Truth와의 IOU 값\n",
    "\n",
    "for num1, i in enumerate(valid_anchor_boxes):\n",
    "    ya1, xa1, ya2, xa2 = i\n",
    "    anchor_area = (ya2-ya1) * (xa2-xa1)\n",
    "    for num2, j in enumerate(bbox):\n",
    "        yb1, xb1, yb2, xb2 = j\n",
    "        box_area = (yb2-yb1) * (xb2-xb1)\n",
    "        \n",
    "        inter_x1 = max([xa1, xb1])\n",
    "        inter_y1 = max([ya1, yb1])\n",
    "        inter_x2 = min([xa2, xb2])\n",
    "        inter_y2 = min([ya2, yb2])\n",
    "        \n",
    "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "            inter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)\n",
    "            iou = inter_area/(box_area + anchor_area - inter_area) # inter_area가 반복됨으로 한 번 빼준다.\n",
    "            \n",
    "        else:\n",
    "            iou = 0\n",
    "            \n",
    "        ious[num1, num2] = iou # 각각의 Anchor box에 대한 2 objects의 iou 값\n",
    "        \n",
    "ious.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 조건에 만족하는 IOU값 및 그 값에 해당하는 Anchor box 선별\n",
    "1. 2개의 Ground-Truth box에 대하여 가장 큰 IOU값과 그 값(2개의 Ground-Truth box)에 해당하는 Anchor box\n",
    "2. 각각의 Anchor box 에서 가장 큰 IOU값과 그 값에 해당하는 Ground-Truth box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:30:07.641880Z",
     "start_time": "2019-11-27T15:30:07.632885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ious.argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:29:38.665513Z",
     "start_time": "2019-11-27T15:29:38.656537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.681304931640625"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ious[:,0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:29:39.065282Z",
     "start_time": "2019-11-27T15:29:39.054288Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6103515625"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ious[:,1].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-27T15:29:45.131799Z",
     "start_time": "2019-11-27T15:29:45.123804Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.68130493, 0.08628624])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ious[2262]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "343px",
    "left": "1005px",
    "right": "20px",
    "top": "120px",
    "width": "341px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
