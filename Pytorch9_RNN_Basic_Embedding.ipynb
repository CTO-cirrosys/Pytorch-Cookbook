{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:10:50.716184Z",
     "start_time": "2018-12-05T18:10:50.691229Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 0, 2, 3, 3]])\n",
      "tensor([1, 0, 2, 3, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.manual_seed(11)\n",
    "\n",
    "letters = ['h','i', 'e', 'l', 'o']\n",
    "\n",
    "# hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]\n",
    "y_data = [1, 0, 2, 3, 3, 4]\n",
    "\n",
    "inputs = torch.LongTensor(x_data)\n",
    "labels = torch.LongTensor(y_data)\n",
    "\n",
    "print(inputs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:10:51.064035Z",
     "start_time": "2018-12-05T18:10:51.016127Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (embedding): Embedding(5, 10)\n",
      "  (rnn): RNN(10, 5, batch_first=True)\n",
      "  (fc): Linear(in_features=5, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class_n = 5\n",
    "input_size = 5\n",
    "hidden_size = 5\n",
    "batch_size = 1\n",
    "embedding_size = 10\n",
    "seq_n = 6\n",
    "layer_n = 1\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.RNN(input_size = embedding_size, hidden_size = 5,\n",
    "                         batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, class_n) # 뭔진 아는데, 이게 왜 있지?\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        h0 = torch.zeros(layer_n, batch_size, hidden_size)\n",
    "        emb = self.embedding(x)\n",
    "        emb = emb.view(batch_size, seq_n, -1)\n",
    "        \n",
    "        output, _ = self.rnn(emb, h0)\n",
    "        output = self.fc(output.view(-1, class_n))\n",
    "        \n",
    "        return output\n",
    "\n",
    "model = Model()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:11:02.079516Z",
     "start_time": "2018-12-05T18:11:02.064540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.5108,  1.0283, -0.3532,  0.1230, -0.1816, -1.4972,  0.1421, -0.5243,\n",
      "         -0.2487, -0.5252],\n",
      "        [ 2.8922, -0.5947,  1.3118,  0.3522, -1.3151, -0.0080,  0.2479,  1.5727,\n",
      "         -1.6395, -1.5925],\n",
      "        [-0.1546, -1.0964,  1.3666,  0.6893, -0.3935,  0.6171,  0.7528,  0.6023,\n",
      "          2.0175, -1.1686],\n",
      "        [-1.3242,  1.1267, -0.2255,  0.5218,  0.5485, -1.5885,  0.5281,  0.8964,\n",
      "          0.9975, -0.2156],\n",
      "        [ 0.0927,  0.3910, -1.0265,  0.0436, -0.9724,  0.2803,  0.5700,  1.4841,\n",
      "         -0.7572, -1.1598]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.3324,  0.1679, -0.0803,  0.1296,  0.4206, -0.4330,  0.0790,  0.2685,\n",
      "          0.0374, -0.2677],\n",
      "        [-0.0824, -0.1333,  0.3466,  0.3995, -0.1756, -0.4392, -0.1417, -0.2468,\n",
      "          0.3809,  0.4222],\n",
      "        [ 0.2309, -0.0382, -0.4312,  0.0321, -0.1722,  0.0050, -0.2094, -0.0466,\n",
      "          0.2527,  0.4416],\n",
      "        [ 0.3677, -0.2432,  0.2912,  0.2909,  0.1795,  0.4241,  0.0356, -0.0538,\n",
      "         -0.2529, -0.0996],\n",
      "        [ 0.1589, -0.3158,  0.0924, -0.1208, -0.3585, -0.0547, -0.1720,  0.3992,\n",
      "         -0.2144,  0.4368]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.2573,  0.4380, -0.0017, -0.0146, -0.3802],\n",
      "        [ 0.1731,  0.2894, -0.0413, -0.0142, -0.0315],\n",
      "        [ 0.2326, -0.2589, -0.0169,  0.0839,  0.1876],\n",
      "        [ 0.3877, -0.3852,  0.0893,  0.0961, -0.1883],\n",
      "        [-0.0832, -0.1031, -0.3527, -0.3601, -0.0252]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2924,  0.4407,  0.0628, -0.3065, -0.1412], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0870,  0.1054,  0.0696, -0.1738,  0.0998], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.3879, -0.3092, -0.1269, -0.2679, -0.1842],\n",
      "        [ 0.4042,  0.2013, -0.0266, -0.0150,  0.3634],\n",
      "        [ 0.1425, -0.0487,  0.1338, -0.0648,  0.0359],\n",
      "        [-0.2881, -0.1811, -0.3487,  0.2594, -0.2045],\n",
      "        [ 0.0469, -0.2403,  0.0416, -0.2168,  0.1947]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4101, -0.2687, -0.3798,  0.0377,  0.3600], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p) # input -> emb lookup table / emb -> hidden / hidden -> output / output -> output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T18:11:06.734438Z",
     "start_time": "2018-12-05T18:11:06.185458Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hohhhh Epoch : 1/100, Loss = 1.833\n",
      "oholhh Epoch : 2/100, Loss = 1.391\n",
      "ehelll Epoch : 3/100, Loss = 1.175\n",
      "ehelll Epoch : 4/100, Loss = 1.021\n",
      "ihilll Epoch : 5/100, Loss = 0.858\n",
      "ihilll Epoch : 6/100, Loss = 0.738\n",
      "ihelll Epoch : 7/100, Loss = 0.657\n",
      "ehelll Epoch : 8/100, Loss = 0.599\n",
      "ehello Epoch : 9/100, Loss = 0.555\n",
      "ehello Epoch : 10/100, Loss = 0.521\n",
      "ihillo Epoch : 11/100, Loss = 0.493\n",
      "ihillo Epoch : 12/100, Loss = 0.466\n",
      "ihillo Epoch : 13/100, Loss = 0.439\n",
      "ihello Epoch : 14/100, Loss = 0.414\n",
      "ehello Epoch : 15/100, Loss = 0.393\n",
      "ehello Epoch : 16/100, Loss = 0.373\n",
      "ehello Epoch : 17/100, Loss = 0.355\n",
      "ihillo Epoch : 18/100, Loss = 0.339\n",
      "ihillo Epoch : 19/100, Loss = 0.325\n",
      "ihillo Epoch : 20/100, Loss = 0.313\n",
      "ihillo Epoch : 21/100, Loss = 0.303\n",
      "ehello Epoch : 22/100, Loss = 0.294\n",
      "ehello Epoch : 23/100, Loss = 0.287\n",
      "ehello Epoch : 24/100, Loss = 0.281\n",
      "ihillo Epoch : 25/100, Loss = 0.275\n",
      "ihillo Epoch : 26/100, Loss = 0.271\n",
      "ihillo Epoch : 27/100, Loss = 0.267\n",
      "ihillo Epoch : 28/100, Loss = 0.263\n",
      "ehello Epoch : 29/100, Loss = 0.260\n",
      "ehello Epoch : 30/100, Loss = 0.258\n",
      "ehello Epoch : 31/100, Loss = 0.255\n",
      "ihillo Epoch : 32/100, Loss = 0.253\n",
      "ihillo Epoch : 33/100, Loss = 0.252\n",
      "ihillo Epoch : 34/100, Loss = 0.250\n",
      "ehello Epoch : 35/100, Loss = 0.249\n",
      "ehello Epoch : 36/100, Loss = 0.248\n",
      "ehello Epoch : 37/100, Loss = 0.247\n",
      "ihillo Epoch : 38/100, Loss = 0.246\n",
      "ihillo Epoch : 39/100, Loss = 0.245\n",
      "ihillo Epoch : 40/100, Loss = 0.245\n",
      "ihillo Epoch : 41/100, Loss = 0.244\n",
      "ehello Epoch : 42/100, Loss = 0.243\n",
      "ehello Epoch : 43/100, Loss = 0.243\n",
      "ihello Epoch : 44/100, Loss = 0.242\n",
      "ihillo Epoch : 45/100, Loss = 0.242\n",
      "ihillo Epoch : 46/100, Loss = 0.241\n",
      "ihillo Epoch : 47/100, Loss = 0.241\n",
      "ihello Epoch : 48/100, Loss = 0.240\n",
      "ihello Epoch : 49/100, Loss = 0.240\n",
      "ihello Epoch : 50/100, Loss = 0.239\n",
      "ihello Epoch : 51/100, Loss = 0.235\n",
      "ihello Epoch : 52/100, Loss = 0.212\n",
      "ihello Epoch : 53/100, Loss = 0.165\n",
      "ihello Epoch : 54/100, Loss = 0.165\n",
      "ihello Epoch : 55/100, Loss = 0.125\n",
      "ihello Epoch : 56/100, Loss = 0.100\n",
      "ihello Epoch : 57/100, Loss = 0.084\n",
      "ihello Epoch : 58/100, Loss = 0.065\n",
      "ihello Epoch : 59/100, Loss = 0.048\n",
      "ihello Epoch : 60/100, Loss = 0.037\n",
      "ihello Epoch : 61/100, Loss = 0.030\n",
      "ihello Epoch : 62/100, Loss = 0.026\n",
      "ihello Epoch : 63/100, Loss = 0.023\n",
      "ihello Epoch : 64/100, Loss = 0.020\n",
      "ihello Epoch : 65/100, Loss = 0.018\n",
      "ihello Epoch : 66/100, Loss = 0.016\n",
      "ihello Epoch : 67/100, Loss = 0.014\n",
      "ihello Epoch : 68/100, Loss = 0.013\n",
      "ihello Epoch : 69/100, Loss = 0.012\n",
      "ihello Epoch : 70/100, Loss = 0.011\n",
      "ihello Epoch : 71/100, Loss = 0.010\n",
      "ihello Epoch : 72/100, Loss = 0.009\n",
      "ihello Epoch : 73/100, Loss = 0.009\n",
      "ihello Epoch : 74/100, Loss = 0.009\n",
      "ihello Epoch : 75/100, Loss = 0.008\n",
      "ihello Epoch : 76/100, Loss = 0.008\n",
      "ihello Epoch : 77/100, Loss = 0.008\n",
      "ihello Epoch : 78/100, Loss = 0.007\n",
      "ihello Epoch : 79/100, Loss = 0.007\n",
      "ihello Epoch : 80/100, Loss = 0.007\n",
      "ihello Epoch : 81/100, Loss = 0.007\n",
      "ihello Epoch : 82/100, Loss = 0.006\n",
      "ihello Epoch : 83/100, Loss = 0.006\n",
      "ihello Epoch : 84/100, Loss = 0.006\n",
      "ihello Epoch : 85/100, Loss = 0.006\n",
      "ihello Epoch : 86/100, Loss = 0.006\n",
      "ihello Epoch : 87/100, Loss = 0.006\n",
      "ihello Epoch : 88/100, Loss = 0.005\n",
      "ihello Epoch : 89/100, Loss = 0.005\n",
      "ihello Epoch : 90/100, Loss = 0.005\n",
      "ihello Epoch : 91/100, Loss = 0.005\n",
      "ihello Epoch : 92/100, Loss = 0.005\n",
      "ihello Epoch : 93/100, Loss = 0.005\n",
      "ihello Epoch : 94/100, Loss = 0.005\n",
      "ihello Epoch : 95/100, Loss = 0.005\n",
      "ihello Epoch : 96/100, Loss = 0.005\n",
      "ihello Epoch : 97/100, Loss = 0.005\n",
      "ihello Epoch : 98/100, Loss = 0.004\n",
      "ihello Epoch : 99/100, Loss = 0.004\n",
      "ihello Epoch : 100/100, Loss = 0.004\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.1)\n",
    "\n",
    "epoch_n = 100\n",
    "\n",
    "for epoch in range(1, epoch_n + 1):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    \n",
    "    output = model(inputs)\n",
    "    loss = loss_function(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    val, idx = output.max(1)\n",
    "    for i in idx:\n",
    "        print(letters[i], end = '')\n",
    "        \n",
    "    print(' Epoch : %d/100, Loss = %1.3f' %(epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 새로 알게 된 것\n",
    "\n",
    "### 1. nn.Embedding(embedding_n, embedding_dim)<br>\n",
    "Look up table, 말 그대로 Tensor에 있는 element를 dim에 맞게 임베딩하고 해당 table을 만들어줌. <br>그냥 index를 설정해준다고 생각하면 된다.\n",
    "- 임베딩 할 요소의 개수(embedding_n)\n",
    "- 임베딩할 때 차원의 수(embedding_dim)\n",
    "\n",
    "### 2. Embedding을 하는 이유\n",
    "- 일단 벡터로 표현된 단어들은 서로 배타적이지 않다. 즉, 연관이 되어있기 때문에 그 이질성을 최소화하기 위해\n",
    "- 또한 원-핫 벡터의 경우 차원의 크기가 기하급수적으로 커지니 차원을 축소하기 위해 임베딩을 한다.\n",
    "- https://www.quora.com/What-does-PyTorch-Embedding-do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
